{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage\n",
    "\n",
    "This notebook showcases the abilities of the ARLIN library. All available methods are \n",
    "shown, but not all will be needed for making decisions. Each method gives insight into \n",
    "different aspects of the trained RL model's policy and the user can use the output\n",
    "analysis to help inlfuence decisions based on the adversary's goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import arlin.dataset.loaders as loaders\n",
    "from arlin.dataset import XRLDataset\n",
    "from arlin.dataset.collectors import SB3PPODataCollector, SB3PPODatapoint\n",
    "\n",
    "from arlin.generation import generate_clusters, generate_embeddings\n",
    "import arlin.analysis.visualization as viz\n",
    "from arlin.analysis import ClusterAnalyzer, LatentAnalyzer\n",
    "from arlin.samdp import SAMDP\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    \"\"\"Create an XRL Dataset from a trained model operating within an environment.\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    \n",
    "    # Load the SB3 model from Huggingface\n",
    "    model = loaders.load_hf_sb_model(repo_id=\"sb3/ppo-LunarLander-v2\",\n",
    "                                     filename=\"ppo-LunarLander-v2.zip\",\n",
    "                                     algo_str=\"ppo\")\n",
    "    \n",
    "    # Create the datapoint collector for SB3 PPO Datapoints with the model's policy\n",
    "    collector = SB3PPODataCollector(datapoint_cls=SB3PPODatapoint,\n",
    "                                    policy=model.policy)\n",
    "    \n",
    "    # Instantiate the XRL Dataset\n",
    "    dataset = XRLDataset(env, collector=collector)\n",
    "    \n",
    "    # Fill the dataset with 50k datapoints and add in additional analysis datapoints\n",
    "    dataset.fill(num_datapoints=50000)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(dataset: XRLDataset):\n",
    "    \"\"\"Generate latent space embeddings from the XRLDataset using T-SNE\"\"\"\n",
    "    \n",
    "    embeddings = generate_embeddings(dataset=dataset,\n",
    "                                     activation_key=\"latent_actors\",\n",
    "                                     perplexity=20,\n",
    "                                     n_train_iter=300,\n",
    "                                     output_dim=2,\n",
    "                                     seed=12345)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def get_clusters(dataset: XRLDataset):\n",
    "    \"\"\"Cluster the latent space embeddings using K-Means and MeanShift\"\"\"\n",
    "    \n",
    "    clusters, _, _, _ = generate_clusters(\n",
    "        dataset,\n",
    "        [\"latent_actors\", \"critic_values\"],\n",
    "        [\"latent_actors\", \"critic_values\", \"rewards\"],\n",
    "        [\"rewards\"],\n",
    "        10,\n",
    "        seed=1234\n",
    "        )\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_latent_analytics(embeddings: np.ndarray, \n",
    "                           clusters: np.ndarray, \n",
    "                           dataset: XRLDataset):\n",
    "    \"\"\"Graph visualizations of different latent space analytics over embeddings.\"\"\"\n",
    "    \n",
    "    # Create a grapher to generate data used for analysis.\n",
    "    grapher = LatentAnalyzer(embeddings, dataset)\n",
    "    \n",
    "    # Embeddings\n",
    "    embeddings_data = grapher.embeddings_graph_data()\n",
    "    # Clusters\n",
    "    cluster_data = grapher.clusters_graph_data(clusters)\n",
    "    # Action decision boundaries\n",
    "    db_data = grapher.decision_boundary_graph_data()\n",
    "    # Initial and terminal states\n",
    "    init_term_data = grapher.initial_terminal_state_data()\n",
    "    # Episode progression\n",
    "    ep_prog_data = grapher.episode_prog_graph_data()\n",
    "    # Greedy action confidence\n",
    "    conf_data = grapher.confidence_data()\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"usage\", \"latent_analytics\")\n",
    "    for data in [(embeddings_data, \"embeddings.png\"),\n",
    "                 (cluster_data, \"clusters.png\"),\n",
    "                 (db_data, \"decision_boundaries.png\"),\n",
    "                 (init_term_data, \"initial_terminal.png\"),\n",
    "                 (ep_prog_data, \"episode_progression.png\"),\n",
    "                 (conf_data, \"confidence.png\")\n",
    "                 ]:\n",
    "        path = os.path.join(base_path, data[1])\n",
    "        \n",
    "        # Graph an individual data graph\n",
    "        viz.graph_individual_data(path, data[0])\n",
    "    \n",
    "    # Graph multiple analytics as subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path,\n",
    "                                           figure_title='Latent Analytics', \n",
    "                                           graph_datas=[db_data, \n",
    "                                                        conf_data, \n",
    "                                                        cluster_data, \n",
    "                                                        ep_prog_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_cluster_analytics(dataset, clusters):\n",
    "    \"\"\"Graph analytics for each cluster\"\"\"\n",
    "    \n",
    "    # Create grapher to graph cluster analytics\n",
    "    grapher = ClusterAnalyzer(dataset, clusters)\n",
    "    \n",
    "    # Mean confidence per cluster\n",
    "    cluster_conf = grapher.cluster_confidence()\n",
    "    # Mean total reward per cluster\n",
    "    cluster_rewards = grapher.cluster_rewards()\n",
    "    # Mean value per cluster\n",
    "    cluster_values = grapher.cluster_values()\n",
    "    \n",
    "    # Graph individual graphs per data\n",
    "    base_path = os.path.join(\".\", \"outputs\", \"usage\", 'cluster_analytics')\n",
    "    for data in [[cluster_conf, 'cluster_confidence.png'], \n",
    "                 [cluster_rewards, 'cluster_rewards.png'],\n",
    "                 [cluster_values, 'cluster_values.png']\n",
    "                 ]:\n",
    "        path = os.path.join(base_path, data[1])\n",
    "        viz.graph_individual_data(path, data[0])\n",
    "    \n",
    "    # Graph multiple subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path, \n",
    "                                           figure_title='Cluster Analytics', \n",
    "                                           graph_datas=[cluster_rewards, cluster_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samdp(clusters: np.ndarray,\n",
    "          dataset: XRLDataset):\n",
    "    \"\"\"Generate a semi-aggregated Markov decision process.\"\"\"\n",
    "    \n",
    "    # Create the SAMDP\n",
    "    samdp = SAMDP(clusters, dataset)\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"usage\", 'samdp')\n",
    "    \n",
    "    # Complete graph with all possible conenctions by action taken\n",
    "    complete_graph = samdp.save_complete_graph(f'{base_path}/samdp_complete.png')\n",
    "    # Complete graph with only the most likely actions taken\n",
    "    likely_graph = samdp.save_likely_graph(f'{base_path}/samdp_likely.png')\n",
    "    # Simplified graph with all possible conenctions (regardless of action taken)\n",
    "    simplified_graph = samdp.save_simplified_graph(f'{base_path}/samdp_simplified.png')\n",
    "    \n",
    "    path_path = os.path.join(base_path, f\"samdp_path_14_9\")\n",
    "    \n",
    "    # Path from cluster 14 to cluster 12\n",
    "    # Action out of cluster 14 shown, all other movements are simplified\n",
    "    samdp.save_paths(14, \n",
    "                     12, \n",
    "                     f'{path_path}.png')\n",
    "    \n",
    "    # Path from cluster 14 to cluster 12\n",
    "    # Show all of the possible actions you can take\n",
    "    samdp.save_paths(14, \n",
    "                     12, \n",
    "                     f'{path_path}_verbose.png', \n",
    "                     verbose=True)\n",
    "    \n",
    "    # Path from cluster 14 to cluster 12\n",
    "    # Only the most likely path is shown\n",
    "    samdp.save_paths(14, \n",
    "                     12,  \n",
    "                     f'{path_path}_bp.png', \n",
    "                     best_path_only=True)\n",
    "    \n",
    "    # Show all paths that lead to cluster 12\n",
    "    # Action into cluster 12 shown, rest is simplified\n",
    "    samdp.save_all_paths_to(12, \n",
    "                            os.path.join(base_path, f\"samdp_paths_to_12.png\"))\n",
    "    \n",
    "    # Show all paths that lead to cluster 12 (all actions shown)\n",
    "    samdp.save_all_paths_to(12, \n",
    "                            os.path.join(base_path, f\"samdp_paths_to_12_verbose.png\"),\n",
    "                            verbose=True)\n",
    "    \n",
    "    # Show all connections into terminal nodes - all paths or only the best paths\n",
    "    samdp.save_terminal_paths(os.path.join(base_path, f\"samdp_terminal_paths.png\"))\n",
    "    samdp.save_terminal_paths(os.path.join(base_path, f\"samdp_terminal_paths_bp.png\"),\n",
    "                              best_path=True)\n",
    "    \n",
    "    # Save a table representation of the SAMDP\n",
    "    samdp.save_txt(f'{base_path}/samdp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset()\n",
    "embeddings = get_embeddings(dataset=dataset)\n",
    "clusters = get_clusters(dataset=dataset)\n",
    "\n",
    "graph_latent_analytics(embeddings=embeddings, clusters=clusters, dataset=dataset)\n",
    "graph_cluster_analytics(dataset=dataset, clusters=clusters)\n",
    "samdp(clusters=clusters, dataset=dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
