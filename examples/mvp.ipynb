{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP Usage and Comparison\n",
    "\n",
    "This notebook showcases the minimum amount of analysis needed to create a more effective\n",
    "adversarial attack against a trained RL policy. The ARLIN library has additional\n",
    "methods to gain more information that may be useful for an adversary, but the following\n",
    "is the minimum needed to identify a terminal state with lower total reward.\n",
    "\n",
    "### Environment\n",
    "\n",
    "For this example, we will use the `LunarLander-v2` environment from `gymnasium`. In this\n",
    "scenario, we are attempting to land a space vehicle with left, right, and vertical \n",
    "thrusters onto a landing pad without crashing into terrain obstacles.\n",
    "\n",
    "### Goal\n",
    "\n",
    "As an adversary, we are aiming to reduce the total reward gained by the agent indicating \n",
    "a failure to achieve the overall mission with high performance. To decrease the \n",
    "possibility of the attack being detected, we want to limit the number of adversarial \n",
    "attacks as much as possible while also ensuring that the attacks performed are not easily\n",
    "detectable by a human observer or automated defense system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import arlin.dataset.loaders as loaders\n",
    "from arlin.dataset import XRLDataset\n",
    "from arlin.dataset.collectors import SB3PPODataCollector, SB3PPODatapoint\n",
    "\n",
    "from arlin.generation import generate_clusters, generate_embeddings\n",
    "import arlin.analysis.visualization as viz\n",
    "from arlin.analysis import ClusterAnalyzer, LatentAnalyzer\n",
    "from arlin.samdp import SAMDP\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    \"\"\"Create an XRL Dataset from a trained model operating within an environment.\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    env.seed(12345)\n",
    "    \n",
    "    # Load the SB3 model from Huggingface\n",
    "    model = loaders.load_hf_sb_model(repo_id=\"sb3/ppo-LunarLander-v2\",\n",
    "                                     filename=\"ppo-LunarLander-v2.zip\",\n",
    "                                     algo_str=\"ppo\")\n",
    "    \n",
    "    # Create the datapoint collector for SB3 PPO Datapoints with the model's policy\n",
    "    collector = SB3PPODataCollector(datapoint_cls=SB3PPODatapoint,\n",
    "                                    policy=model.policy)\n",
    "    \n",
    "    # Instantiate the XRL Dataset\n",
    "    dataset = XRLDataset(env, collector=collector)\n",
    "    \n",
    "    # Fill the dataset with 50k datapoints and add in additional analysis datapoints\n",
    "    dataset.fill(num_datapoints=50000)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(dataset: XRLDataset):\n",
    "    \"\"\"Generate latent space embeddings from the XRLDataset using T-SNE\"\"\"\n",
    "    \n",
    "    embeddings = generate_embeddings(dataset=dataset,\n",
    "                                     activation_key=\"latent_actors\",\n",
    "                                     perplexity=500,\n",
    "                                     n_train_iter=4000,\n",
    "                                     output_dim=2,\n",
    "                                     seed=12345)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def get_clusters(dataset: XRLDataset, embeddings: np.ndarray):\n",
    "    \"\"\"Cluster the latent space embeddings using K-Means and MeanShift\"\"\"\n",
    "    \n",
    "    clusters = generate_clusters(dataset=dataset,\n",
    "                                 embeddings=embeddings,\n",
    "                                 num_clusters=14,\n",
    "                                 seed=12345)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_latent_analytics(embeddings: np.ndarray, \n",
    "                           clusters: np.ndarray, \n",
    "                           dataset: XRLDataset):\n",
    "    \"\"\"Graph visualizations of different latent space analytics over embeddings.\"\"\"\n",
    "    \n",
    "    # Create a grapher to generate data used for analysis.\n",
    "    grapher = LatentAnalyzer(embeddings, dataset)\n",
    "    \n",
    "    # Clusters\n",
    "    cluster_data = grapher.clusters_graph_data(clusters)\n",
    "    # Episode progression\n",
    "    ep_prog_data = grapher.episode_prog_graph_data()\n",
    "    # Greedy action confidence\n",
    "    conf_data = grapher.confidence_data()\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"mvp\", \"latent_analytics\")\n",
    "    for data in [(cluster_data, \"clusters.png\"),\n",
    "                 (ep_prog_data, \"episode_progression.png\"),\n",
    "                 (conf_data, \"confidence.png\")\n",
    "                 ]:\n",
    "        path = os.path.join(base_path, data[1])\n",
    "        \n",
    "        # Graph an individual data graph\n",
    "        viz.graph_individual_data(path, data[0])\n",
    "    \n",
    "    # Graph multiple analytics as subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path,\n",
    "                                           figure_title='Latent Analytics', \n",
    "                                           graph_datas=[conf_data, \n",
    "                                                        cluster_data, \n",
    "                                                        ep_prog_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_cluster_analytics(dataset, clusters):\n",
    "    \"\"\"Graph analytics for each cluster\"\"\"\n",
    "    \n",
    "    # Create grapher to graph cluster analytics\n",
    "    grapher = ClusterAnalyzer(dataset, clusters)\n",
    "    \n",
    "    # Mean confidence per cluster\n",
    "    cluster_conf = grapher.cluster_confidence()\n",
    "    # Mean total reward per cluster\n",
    "    cluster_rewards = grapher.cluster_rewards()\n",
    "    # Mean value per cluster\n",
    "    cluster_values = grapher.cluster_values()\n",
    "    \n",
    "    # Graph individual graphs per data\n",
    "    base_path = os.path.join(\".\", \"outputs\", \"mvp\", 'cluster_analytics')\n",
    "    for data in [[cluster_conf, 'cluster_confidence.png'], \n",
    "                 [cluster_rewards, 'cluster_rewards.png'],\n",
    "                 [cluster_values, 'cluster_values.png']\n",
    "                 ]:\n",
    "        path = os.path.join(base_path, data[1])\n",
    "        viz.graph_individual_data(path, data[0])\n",
    "    \n",
    "    # Graph multiple subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path, \n",
    "                                           figure_title='Cluster Analytics', \n",
    "                                           graph_datas=[cluster_rewards, \n",
    "                                                        cluster_conf,\n",
    "                                                        cluster_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samdp(clusters: np.ndarray,\n",
    "          dataset: XRLDataset):\n",
    "    \"\"\"Generate a semi-aggregated Markov decision process.\"\"\"\n",
    "    \n",
    "    # Create the SAMDP\n",
    "    samdp = SAMDP(clusters, dataset)\n",
    "    \n",
    "    base_path = os.path.join(\"./outputs\", 'samdp')\n",
    "    \n",
    "    # Simplified graph with all possible conenctions (regardless of action taken)\n",
    "    simplified_graph = samdp.save_simplified_graph(f'{base_path}/samdp_simplified.png')\n",
    "    \n",
    "    path_path = os.path.join(base_path, f\"samdp_path_14_9\")\n",
    "    \n",
    "    # Path from cluster 14 to cluster 12\n",
    "    # Action out of cluster 14 shown, all other movements are simplified\n",
    "    samdp.save_paths(14, \n",
    "                     12, \n",
    "                     f'{path_path}.png')\n",
    "    \n",
    "    # Path from cluster 14 to cluster 12\n",
    "    # Show all of the possible actions you can take\n",
    "    samdp.save_paths(14, \n",
    "                     12, \n",
    "                     f'{path_path}_verbose.png', \n",
    "                     verbose=True)\n",
    "    \n",
    "    # Path from cluster 14 to cluster 12\n",
    "    # Only the most likely path is shown\n",
    "    samdp.save_paths(14, \n",
    "                     12,  \n",
    "                     f'{path_path}_bp.png', \n",
    "                     best_path_only=True)\n",
    "    \n",
    "    # Show all paths that lead to cluster 12\n",
    "    # Action into cluster 12 shown, rest is simplified\n",
    "    samdp.save_all_paths_to(12, \n",
    "                            os.path.join(base_path, f\"samdp_paths_to_12.png\"))\n",
    "    \n",
    "    # Show all paths that lead to cluster 12 (all actions shown)\n",
    "    samdp.save_all_paths_to(12, \n",
    "                            os.path.join(base_path, f\"samdp_paths_to_12_verbose.png\"),\n",
    "                            verbose=True)\n",
    "    \n",
    "    # Save a table representation of the SAMDP\n",
    "    samdp.save_txt(f'{base_path}/samdp.txt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
