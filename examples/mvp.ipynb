{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP Usage and Comparison\n",
    "\n",
    "This notebook showcases the minimum amount of analysis needed to create a more effective\n",
    "adversarial attack against a trained RL policy. The ARLIN library has additional\n",
    "methods to gain more information that may be useful for an adversary, but the following\n",
    "is the minimum needed to identify a terminal state with lower total reward.\n",
    "\n",
    "### Environment\n",
    "\n",
    "For this example, we will use the `LunarLander-v2` environment from `gymnasium`. In this\n",
    "scenario, we are attempting to land a space vehicle with left, right, and vertical \n",
    "thrusters onto a landing pad without crashing into terrain obstacles.\n",
    "\n",
    "### Goal\n",
    "\n",
    "As an adversary, we are aiming to reduce the total reward gained by the agent indicating \n",
    "a failure to achieve the overall mission with high performance. To decrease the \n",
    "possibility of the attack being detected, we want to limit the number of adversarial \n",
    "attacks as much as possible while also ensuring that the attacks performed are not easily\n",
    "detectable by a human observer or automated defense system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import arlin.dataset.loaders as loaders\n",
    "from arlin.dataset import XRLDataset\n",
    "from arlin.dataset.collectors import SB3PPODataCollector, SB3PPODatapoint\n",
    "\n",
    "from arlin.generation import generate_clusters, generate_embeddings\n",
    "import arlin.analysis.visualization as viz\n",
    "from arlin.analysis import ClusterAnalyzer, LatentAnalyzer\n",
    "from arlin.samdp import SAMDP\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    \"\"\"Create an XRL Dataset from a trained model operating within an environment.\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    \n",
    "    # Load the SB3 model from Huggingface\n",
    "    model = loaders.load_hf_sb_model(repo_id=\"sb3/ppo-LunarLander-v2\",\n",
    "                                     filename=\"ppo-LunarLander-v2.zip\",\n",
    "                                     algo_str=\"ppo\")\n",
    "    \n",
    "    # Create the datapoint collector for SB3 PPO Datapoints with the model's policy\n",
    "    collector = SB3PPODataCollector(datapoint_cls=SB3PPODatapoint,\n",
    "                                    policy=model.policy)\n",
    "    \n",
    "    # Instantiate the XRL Dataset\n",
    "    dataset = XRLDataset(env, collector=collector)\n",
    "    \n",
    "    # Fill the dataset with 50k datapoints and add in additional analysis datapoints\n",
    "    dataset.fill(num_datapoints=50000)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(dataset: XRLDataset):\n",
    "    \"\"\"Generate latent space embeddings from the XRLDataset using T-SNE\"\"\"\n",
    "    \n",
    "    embeddings = generate_embeddings(dataset=dataset,\n",
    "                                     activation_key=\"latent_actors\",\n",
    "                                     perplexity=50000,\n",
    "                                     n_train_iter=4000,\n",
    "                                     output_dim=2,\n",
    "                                     seed=12345)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def get_clusters(dataset: XRLDataset, embeddings: np.ndarray):\n",
    "    \"\"\"Cluster the latent space embeddings using K-Means and MeanShift\"\"\"\n",
    "    \n",
    "    clusters = generate_clusters(dataset=dataset,\n",
    "                                 embeddings=embeddings,\n",
    "                                 num_clusters=14,\n",
    "                                 seed=12345)\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_latent_analytics(embeddings: np.ndarray, \n",
    "                           clusters: np.ndarray, \n",
    "                           dataset: XRLDataset):\n",
    "    \"\"\"Graph visualizations of different latent space analytics over embeddings.\"\"\"\n",
    "    \n",
    "    # Create a grapher to generate data used for analysis.\n",
    "    grapher = LatentAnalyzer(embeddings, dataset)\n",
    "    \n",
    "    # Clusters\n",
    "    cluster_data = grapher.clusters_graph_data(clusters)\n",
    "    # Episode progression\n",
    "    ep_prog_data = grapher.episode_prog_graph_data()\n",
    "    # Greedy action confidence\n",
    "    conf_data = grapher.confidence_data()\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"mvp\", \"latent_analytics\")\n",
    "    for data in [(cluster_data, \"clusters.png\"),\n",
    "                 (ep_prog_data, \"episode_progression.png\"),\n",
    "                 (conf_data, \"confidence.png\")\n",
    "                 ]:\n",
    "        path = os.path.join(base_path, data[1])\n",
    "        \n",
    "        # Graph an individual data graph\n",
    "        viz.graph_individual_data(path, data[0])\n",
    "    \n",
    "    # Graph multiple analytics as subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path,\n",
    "                                           figure_title='Latent Analytics', \n",
    "                                           graph_datas=[conf_data, \n",
    "                                                        cluster_data, \n",
    "                                                        ep_prog_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_cluster_analytics(dataset, clusters):\n",
    "    \"\"\"Graph analytics for each cluster\"\"\"\n",
    "    \n",
    "    # Create grapher to graph cluster analytics\n",
    "    grapher = ClusterAnalyzer(dataset, clusters)\n",
    "    \n",
    "    grapher.cluster_state_analysis(19,\n",
    "                                   gym.make('LunarLander-v2'), \n",
    "                                   os.path.join(\".\", \"outputs\", \"mvp\", \"cluster_state_analysis\"))\n",
    "    \n",
    "    # Mean confidence per cluster\n",
    "    cluster_conf = grapher.cluster_confidence()\n",
    "    # Mean total reward per cluster\n",
    "    cluster_rewards = grapher.cluster_rewards()\n",
    "    # Mean value per cluster\n",
    "    cluster_values = grapher.cluster_values()\n",
    "    \n",
    "    # Graph individual graphs per data\n",
    "    base_path = os.path.join(\".\", \"outputs\", \"mvp\", 'cluster_analytics')\n",
    "    for data in [[cluster_conf, 'cluster_confidence.png'], \n",
    "                 [cluster_rewards, 'cluster_rewards.png'],\n",
    "                 [cluster_values, 'cluster_values.png']\n",
    "                 ]:\n",
    "        path = os.path.join(base_path, data[1])\n",
    "        viz.graph_individual_data(path, data[0])\n",
    "    \n",
    "    # Graph multiple subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path, \n",
    "                                           figure_title='Cluster Analytics', \n",
    "                                           graph_datas=[cluster_rewards, \n",
    "                                                        cluster_conf,\n",
    "                                                        cluster_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msamdp\u001b[39m(clusters: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m      2\u001b[0m           dataset: XRLDataset):\n\u001b[1;32m      3\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate a semi-aggregated Markov decision process.\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m# Create the SAMDP\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def samdp(clusters: np.ndarray,\n",
    "          dataset: XRLDataset):\n",
    "    \"\"\"Generate a semi-aggregated Markov decision process.\"\"\"\n",
    "    \n",
    "    # Create the SAMDP\n",
    "    samdp = SAMDP(clusters, dataset)\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"mvp\", 'samdp')\n",
    "    \n",
    "    # Simplified graph with all possible conenctions (regardless of action taken)\n",
    "    simplified_graph = samdp.save_simplified_graph(f'{base_path}/samdp_simplified.png')\n",
    "    \n",
    "    path_path = os.path.join(base_path, f\"samdp_path_15_19\")\n",
    "    \n",
    "    # Path from cluster 15 to cluster 19\n",
    "    # Action out of cluster 15 shown, all other movements are simplified\n",
    "    samdp.save_paths(15, \n",
    "                     19, \n",
    "                     f'{path_path}.png')\n",
    "    \n",
    "    # Path from cluster 15 to cluster 19\n",
    "    # Only the most likely path is shown\n",
    "    samdp.save_paths(15, \n",
    "                     19,  \n",
    "                     f'{path_path}_bp.png', \n",
    "                     best_path_only=True)\n",
    "    \n",
    "    # Show all paths that lead to cluster 12\n",
    "    # Action into cluster 12 shown, rest is simplified\n",
    "    samdp.save_all_paths_to(19, \n",
    "                            os.path.join(base_path, f\"samdp_paths_to_12.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model sb3/ppo-LunarLander-v2/ppo-LunarLander-v2.zip from huggingface...\n",
      "INFO:root:Loading ppo model ppo-LunarLander-v2.zip with stable_baselines3...\n",
      "INFO:root:Generating embeddings from dataset.latent_actors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 1501 nearest neighbors...\n",
      "[t-SNE] Indexed 50339 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 50339 samples in 7.659s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 30000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 31000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 32000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 33000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 34000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 35000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 36000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 37000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 38000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 39000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 40000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 41000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 42000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 43000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 44000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 45000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 46000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 47000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 48000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 49000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 50000 / 50339\n",
      "[t-SNE] Computed conditional probabilities for sample 50339 / 50339\n",
      "[t-SNE] Mean sigma: 0.100680\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 72.089310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\tSuccessfully generated embeddings in 15.670834064483643 minutes.\n",
      "INFO:root:Generating 14 clusters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 4000 iterations: 1.393429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\tSuccessfully generated clusters in 1.5578913688659668 seconds.\n",
      "INFO:root:Saving individual graph png to ./outputs/mvp/latent_analytics/clusters.png...\n",
      "INFO:root:Saving individual graph png to ./outputs/mvp/latent_analytics/episode_progression.png...\n",
      "INFO:root:Saving individual graph png to ./outputs/mvp/latent_analytics/confidence.png...\n",
      "INFO:root:Saving combination graph png to ./outputs/mvp/latent_analytics/combined_analytics.png...\n",
      "INFO:root:Saving individual graph png to ./outputs/mvp/cluster_analytics/cluster_confidence.png...\n",
      "INFO:root:Saving individual graph png to ./outputs/mvp/cluster_analytics/cluster_rewards.png...\n",
      "INFO:root:Saving individual graph png to ./outputs/mvp/cluster_analytics/cluster_values.png...\n",
      "INFO:root:Saving combination graph png to ./outputs/mvp/cluster_analytics/combined_analytics.png...\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset()\n",
    "embeddings = get_embeddings(dataset)\n",
    "clusters = get_clusters(dataset, embeddings)\n",
    "\n",
    "graph_latent_analytics(embeddings, clusters, dataset)\n",
    "graph_cluster_analytics(dataset, clusters)\n",
    "samdp(clusters, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
