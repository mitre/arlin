{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Attack\n",
    "\n",
    "This notebook looks into how ARLIN can be used to create more effective adversarial attacks. The notebook will show the average reward gained and total number of attacks in various attack scenarios against the same trained RL model:\n",
    "\n",
    "- Random action every step\n",
    "- Worst-case action every step\n",
    "- Worst-case action every 10 steps\n",
    "- Least-preferred action based on threshold (https://arxiv.org/pdf/1703.06748.pdf)\n",
    "- ARLIN-informed actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import arlin.dataset.loaders as loaders\n",
    "from arlin.dataset import XRLDataset\n",
    "from arlin.dataset.collectors import SB3PPODataCollector, SB3PPODatapoint\n",
    "\n",
    "from arlin.generation import generate_clusters, generate_embeddings\n",
    "import arlin.analysis.visualization as viz\n",
    "from arlin.analysis import ClusterAnalyzer, LatentAnalyzer\n",
    "from arlin.samdp import SAMDP\n",
    "import arlin.utils.saving_loading as sl_utils\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model sb3/ppo-LunarLander-v2/ppo-LunarLander-v2.zip from huggingface...\n",
      "INFO:root:Loading ppo model ppo-LunarLander-v2.zip with stable_baselines3...\n",
      "INFO:root:Loading ppo model adv_ppo_lunar.zip with stable_baselines3...\n",
      "INFO:root:Generating embeddings from dataset.latent_actors.\n",
      "Performing t-SNE using 4 cores.\n",
      "Using no_dims = 2, perplexity = 225.000000, and theta = 0.500000\n",
      "Computing input similarities...\n",
      "Building tree...\n",
      " - point 5031 of 50319\n",
      " - point 10062 of 50319\n",
      " - point 15093 of 50319\n",
      " - point 20124 of 50319\n",
      " - point 25155 of 50319\n",
      " - point 30186 of 50319\n",
      " - point 35217 of 50319\n",
      " - point 40248 of 50319\n",
      " - point 45279 of 50319\n",
      " - point 50310 of 50319\n",
      "Done in 37.00 seconds (sparsity = 0.018327)!\n",
      "Learning embedding...\n",
      "Iteration 51: error is 93.263221 (50 iterations in 22.00 seconds)\n",
      "Iteration 101: error is 88.675318 (50 iterations in 18.00 seconds)\n",
      "Iteration 151: error is 82.382958 (50 iterations in 17.00 seconds)\n",
      "Iteration 201: error is 80.527979 (50 iterations in 17.00 seconds)\n",
      "Iteration 251: error is 79.967933 (50 iterations in 17.00 seconds)\n",
      "Iteration 301: error is 3.013125 (50 iterations in 16.00 seconds)\n",
      "Iteration 351: error is 2.703363 (50 iterations in 17.00 seconds)\n",
      "Iteration 401: error is 2.524324 (50 iterations in 16.00 seconds)\n",
      "Iteration 451: error is 2.397601 (50 iterations in 17.00 seconds)\n",
      "Iteration 501: error is 2.301746 (50 iterations in 17.00 seconds)\n",
      "Iteration 551: error is 2.225680 (50 iterations in 16.00 seconds)\n",
      "Iteration 601: error is 2.163335 (50 iterations in 17.00 seconds)\n",
      "Iteration 651: error is 2.111389 (50 iterations in 18.00 seconds)\n",
      "Iteration 701: error is 2.067065 (50 iterations in 16.00 seconds)\n",
      "Iteration 751: error is 2.029465 (50 iterations in 17.00 seconds)\n",
      "Iteration 801: error is 1.996858 (50 iterations in 17.00 seconds)\n",
      "Iteration 851: error is 1.968138 (50 iterations in 17.00 seconds)\n",
      "Iteration 901: error is 1.942858 (50 iterations in 16.00 seconds)\n",
      "Iteration 951: error is 1.920338 (50 iterations in 17.00 seconds)\n",
      "Iteration 1001: error is 1.900291 (50 iterations in 17.00 seconds)\n",
      "Iteration 1051: error is 1.883008 (50 iterations in 16.00 seconds)\n",
      "Iteration 1101: error is 1.867547 (50 iterations in 17.00 seconds)\n",
      "Iteration 1151: error is 1.853959 (50 iterations in 16.00 seconds)\n",
      "Iteration 1201: error is 1.841889 (50 iterations in 17.00 seconds)\n",
      "Iteration 1251: error is 1.831127 (50 iterations in 16.00 seconds)\n",
      "Iteration 1301: error is 1.821583 (50 iterations in 17.00 seconds)\n",
      "Iteration 1351: error is 1.812459 (50 iterations in 17.00 seconds)\n",
      "Iteration 1401: error is 1.804419 (50 iterations in 16.00 seconds)\n",
      "Iteration 1451: error is 1.797254 (50 iterations in 17.00 seconds)\n",
      "Iteration 1501: error is 1.790455 (50 iterations in 16.00 seconds)\n",
      "Iteration 1551: error is 1.785182 (50 iterations in 17.00 seconds)\n",
      "Iteration 1601: error is 1.780813 (50 iterations in 17.00 seconds)\n",
      "Iteration 1651: error is 1.777166 (50 iterations in 16.00 seconds)\n",
      "Iteration 1701: error is 1.773901 (50 iterations in 17.00 seconds)\n",
      "Iteration 1751: error is 1.770906 (50 iterations in 17.00 seconds)\n",
      "Iteration 1801: error is 1.769052 (50 iterations in 17.00 seconds)\n",
      "Iteration 1851: error is 1.767583 (50 iterations in 16.00 seconds)\n",
      "Iteration 1901: error is 1.766311 (50 iterations in 17.00 seconds)\n",
      "Iteration 1951: error is 1.765021 (50 iterations in 17.00 seconds)\n",
      "Iteration 2001: error is 1.764009 (50 iterations in 17.00 seconds)\n",
      "Iteration 2051: error is 1.763204 (50 iterations in 17.00 seconds)\n",
      "Iteration 2101: error is 1.762875 (50 iterations in 17.00 seconds)\n",
      "Iteration 2151: error is 1.762475 (50 iterations in 16.00 seconds)\n",
      "Iteration 2201: error is 1.762108 (50 iterations in 17.00 seconds)\n",
      "Iteration 2251: error is 1.761564 (50 iterations in 17.00 seconds)\n",
      "Iteration 2301: error is 1.760987 (50 iterations in 17.00 seconds)\n",
      "Iteration 2351: error is 1.760464 (50 iterations in 18.00 seconds)\n",
      "Iteration 2401: error is 1.759791 (50 iterations in 16.00 seconds)\n",
      "Iteration 2451: error is 1.759032 (50 iterations in 17.00 seconds)\n",
      "Iteration 2501: error is 1.758246 (50 iterations in 17.00 seconds)\n",
      "Iteration 2551: error is 1.757227 (50 iterations in 18.00 seconds)\n",
      "Iteration 2601: error is 1.756038 (50 iterations in 17.00 seconds)\n",
      "Iteration 2651: error is 1.755111 (50 iterations in 17.00 seconds)\n",
      "Iteration 2701: error is 1.754134 (50 iterations in 16.00 seconds)\n",
      "Iteration 2751: error is 1.753177 (50 iterations in 17.00 seconds)\n",
      "Iteration 2801: error is 1.752113 (50 iterations in 18.00 seconds)\n",
      "Iteration 2851: error is 1.751195 (50 iterations in 17.00 seconds)\n",
      "Iteration 2901: error is 1.750174 (50 iterations in 17.00 seconds)\n",
      "Iteration 2951: error is 1.749094 (50 iterations in 18.00 seconds)\n",
      "Iteration 3001: error is 1.747960 (50 iterations in 17.00 seconds)\n",
      "Iteration 3051: error is 1.747006 (50 iterations in 17.00 seconds)\n",
      "Iteration 3101: error is 1.745855 (50 iterations in 18.00 seconds)\n",
      "Iteration 3151: error is 1.744672 (50 iterations in 16.00 seconds)\n",
      "Iteration 3201: error is 1.743475 (50 iterations in 18.00 seconds)\n",
      "Iteration 3251: error is 1.742206 (50 iterations in 17.00 seconds)\n",
      "Iteration 3301: error is 1.741033 (50 iterations in 18.00 seconds)\n",
      "Iteration 3351: error is 1.739961 (50 iterations in 17.00 seconds)\n",
      "Iteration 3401: error is 1.738739 (50 iterations in 17.00 seconds)\n",
      "Iteration 3451: error is 1.737458 (50 iterations in 18.00 seconds)\n",
      "Iteration 3501: error is 1.736235 (50 iterations in 17.00 seconds)\n",
      "Iteration 3551: error is 1.734819 (50 iterations in 17.00 seconds)\n",
      "Iteration 3601: error is 1.733397 (50 iterations in 18.00 seconds)\n",
      "Iteration 3651: error is 1.731961 (50 iterations in 17.00 seconds)\n",
      "Iteration 3701: error is 1.730591 (50 iterations in 18.00 seconds)\n",
      "Iteration 3751: error is 1.729345 (50 iterations in 17.00 seconds)\n",
      "Iteration 3801: error is 1.728156 (50 iterations in 17.00 seconds)\n",
      "Iteration 3851: error is 1.727018 (50 iterations in 18.00 seconds)\n",
      "Iteration 3901: error is 1.725966 (50 iterations in 17.00 seconds)\n",
      "Iteration 3951: error is 1.724952 (50 iterations in 18.00 seconds)\n",
      "Iteration 4000: error is 1.723955 (50 iterations in 17.00 seconds)\n",
      "Fitting performed in 1363.00 seconds.\n",
      "INFO:root:\tSuccessfully generated embeddings in 22.372931241989136 minutes.\n",
      "INFO:root:Saving data to ./data/LunarLander-50000-Embeddings.npz...\n",
      "INFO:root:\tData saved successfully.\n",
      "INFO:root:Generating 14 clusters.\n",
      "INFO:root:\tSuccessfully generated clusters in 4.117976188659668 seconds.\n",
      "INFO:root:Saving data to ./data/LunarLander-50000-Clusters.npz...\n",
      "INFO:root:\tData saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode='rgb_array')\n",
    "\n",
    "# Load the SB3 model from Huggingface\n",
    "model = loaders.load_hf_sb_model(repo_id=\"sb3/ppo-LunarLander-v2\",\n",
    "                                 filename=\"ppo-LunarLander-v2.zip\",\n",
    "                                 algo_str=\"ppo\")\n",
    "\n",
    "adv_model = loaders.load_sb_model('./models/adv_ppo_lunar.zip', 'ppo')\n",
    "\n",
    "# Create the datapoint collector for SB3 PPO Datapoints with the model's policy\n",
    "collector = SB3PPODataCollector(datapoint_cls=SB3PPODatapoint,\n",
    "                                policy=model.policy)\n",
    "\n",
    "# Instantiate the XRL Dataset\n",
    "dataset = XRLDataset(env, collector=collector)\n",
    "\n",
    "# dataset.fill(num_datapoints=50000)\n",
    "# dataset.save(file_path='./data/LunarLander-50000.npz')\n",
    "\n",
    "# Load the dataset, embeddings, and clusters\n",
    "dataset.load('./data/LunarLander-50000.npz')\n",
    "embeddings = sl_utils.load_data(file_path='./data/LunarLander-50000-Embeddings.npz')\n",
    "clusters = sl_utils.load_data(file_path='./data/LunarLander-50000-Clusters.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARLIN Usage\n",
    "\n",
    "Let's use the ARLIN toolkit to identify when we should be performing our adversarial\n",
    "attack, and which actions we should target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_latent_analytics(embeddings: np.ndarray, \n",
    "                           clusters: np.ndarray, \n",
    "                           dataset: XRLDataset):\n",
    "    \"\"\"Graph visualizations of different latent space analytics over embeddings.\"\"\"\n",
    "    \n",
    "    # Create a grapher to generate data used for analysis.\n",
    "    grapher = LatentAnalyzer(embeddings, dataset)\n",
    "    \n",
    "    # Clusters\n",
    "    cluster_data = grapher.clusters_graph_data(clusters)\n",
    "    # Episode progression\n",
    "    ep_prog_data = grapher.episode_prog_graph_data()\n",
    "    # Greedy action confidence\n",
    "    conf_data = grapher.confidence_data()\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", \"latent_analytics\")\n",
    "    \n",
    "    # Graph multiple analytics as subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path,\n",
    "                                           figure_title='Latent Analytics', \n",
    "                                           graph_datas=[conf_data, \n",
    "                                                        cluster_data, \n",
    "                                                        ep_prog_data])\n",
    "\n",
    "def graph_cluster_analytics(dataset, clusters):\n",
    "    \"\"\"Graph analytics for each cluster\"\"\"\n",
    "    \n",
    "    # Create grapher to graph cluster analytics\n",
    "    grapher = ClusterAnalyzer(dataset, clusters)\n",
    "    \n",
    "    grapher.cluster_state_analysis(19,\n",
    "                                   gym.make('LunarLander-v2'), \n",
    "                                   os.path.join(\".\", \"outputs\", \"attack\", \"cluster_state_analysis\"))\n",
    "    \n",
    "    grapher.cluster_state_analysis(15,\n",
    "                                   gym.make('LunarLander-v2'), \n",
    "                                   os.path.join(\".\", \"outputs\", \"attack\", \"cluster_state_analysis\"))\n",
    "    \n",
    "    # Mean confidence per cluster\n",
    "    cluster_conf = grapher.cluster_confidence()\n",
    "    # Mean total reward per cluster\n",
    "    cluster_rewards = grapher.cluster_rewards()\n",
    "    # Mean value per cluster\n",
    "    cluster_values = grapher.cluster_values()\n",
    "    \n",
    "    # Graph individual graphs per data\n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", 'cluster_analytics')\n",
    "    \n",
    "    # Graph multiple subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path, \n",
    "                                           figure_title='Cluster Analytics', \n",
    "                                           graph_datas=[cluster_conf,\n",
    "                                                        cluster_values,\n",
    "                                                        cluster_rewards])\n",
    "\n",
    "def samdp(clusters: np.ndarray,\n",
    "          dataset: XRLDataset):\n",
    "    \"\"\"Generate a semi-aggregated Markov decision process.\"\"\"\n",
    "    \n",
    "    # Create the SAMDP\n",
    "    samdp = SAMDP(clusters, dataset)\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", 'samdp')\n",
    "    \n",
    "    # Simplified graph with all possible conenctions (regardless of action taken)\n",
    "    simplified_graph = samdp.save_simplified_graph(f'{base_path}/samdp_simplified.png')\n",
    "    \n",
    "    path_path = os.path.join(base_path, f\"samdp_path_15_19\")\n",
    "    \n",
    "    # Path from cluster 15 to cluster 19\n",
    "    # Action out of cluster 15 shown, all other movements are simplified\n",
    "    samdp.save_paths(15, \n",
    "                     19, \n",
    "                     f'{path_path}.png')\n",
    "    \n",
    "    # Path from cluster 15 to cluster 19\n",
    "    # Only the most likely path is shown\n",
    "    samdp.save_paths(15, \n",
    "                     19,  \n",
    "                     f'{path_path}_bp.png', \n",
    "                     best_path_only=True)\n",
    "    \n",
    "    # Show all paths that lead to cluster 12\n",
    "    # Action into cluster 12 shown, rest is simplified\n",
    "    samdp.save_all_paths_to(19, \n",
    "                            os.path.join(base_path, f\"samdp_paths_to_12.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_latent_analytics(embeddings, clusters, dataset)\n",
    "# graph_cluster_analytics(dataset, clusters)\n",
    "# samdp(clusters, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_attack(model_type: str, \n",
    "                  timestep: int, \n",
    "                  freq: int = 0,\n",
    "                  preference: float = 0, \n",
    "                  threshold: float = 1.0) -> bool:\n",
    "    \"\"\"Check whether or not we should attack at the given timestep.\n",
    "\n",
    "    Args:\n",
    "        model_type (str): Type of model we want to run.\n",
    "        timestep (int): Current timestep\n",
    "        freq (int, optional): Frequency of attack. Defaults to 0.\n",
    "        preference (float, optional): Delta between most and least preferred action.\n",
    "            Defaults to 0.\n",
    "        threshold (float, optional): Threshold for preference attack. Defaults to 1.0.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If invalid model type is given.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether or not to attack\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_type == 'baseline':\n",
    "        return False\n",
    "    elif model_type == 'random' or model_type == 'adversarial':\n",
    "        if timestep % freq == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif model_type == 'preference':\n",
    "        if preference > threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model_type {model_type} given.\")\n",
    "\n",
    "def get_action(obs: np.ndarray,\n",
    "               model_type: str, \n",
    "               timestep: int, \n",
    "               freq: int = 0,\n",
    "               preference: float = 0, \n",
    "               threshold: float = 1.0) -> int:\n",
    "    \"\"\"Get the action to take at the given timestep.\n",
    "\n",
    "    Args:\n",
    "        obs (np.ndarray): Current observation from the agent.\n",
    "        model_type (str): Type of model we want to run.\n",
    "        timestep (int): Current timestep\n",
    "        freq (int, optional): Frequency of attack. Defaults to 0.\n",
    "        preference (float, optional): Delta between most and least preferred action.\n",
    "            Defaults to 0.\n",
    "        threshold (float, optional): Threshold for preference attack. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        int: Action value to take.\n",
    "    \"\"\"\n",
    "    \n",
    "    if should_attack(model_type, timestep, freq, preference, threshold):\n",
    "        if model_type == 'random':\n",
    "            rng = np.random.default_rng(12345)\n",
    "            action = rng.integers(low=0, high=env.action_space.n, size=1).item()\n",
    "        else:\n",
    "            action, _ = adv_model.predict(obs, deterministic=True)\n",
    "    else:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_reward(model_type: str ='baseline', \n",
    "                       freq: int = 0,\n",
    "                       threshold: int = 1) -> float:\n",
    "    \"\"\"Average reward over 10 episodes while the model is being attacked.\n",
    "    \n",
    "    Attacks happen at the given freq and come from the given model type. \n",
    "        - Baseline does not include any adversarial attacks.\n",
    "        - Random chooses the action randomly.\n",
    "        - Adversarial chooses the worst possible action at that point in time.\n",
    "        - Preference chooses the least preferred action when the pref is above a threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for ep in range(10):\n",
    "        obs, _ = env.reset(seed=1234 + ep)\n",
    "        done = False\n",
    "        step = 0\n",
    "        ep_rew = 0\n",
    "        \n",
    "        while not done:\n",
    "            internal_data, _ = collector.collect_internal_data(obs)\n",
    "            probs = internal_data.dist_probs\n",
    "            preference = probs.max() - probs.min()\n",
    "            \n",
    "            action = get_action(obs, model_type, step, freq, preference, threshold)\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            ep_rew += reward\n",
    "            done = terminated or truncated\n",
    "            step += 1\n",
    "            \n",
    "        episode_rewards.append(ep_rew)\n",
    "    \n",
    "    return sum(episode_rewards) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Avg Reward: 247.46209077695022\n",
      "Random Action Every 1 Avg Reward: -588.4916522579238\n",
      "Random Action Every 10 Avg Reward: 138.24303406750838\n",
      "Worst Action Every 1 Avg Reward: -594.7959537977795\n",
      "Worst Action Every 10 Avg Reward: 138.16942678929826\n",
      "Preference at .50 Avg Reward: -547.7386702780462\n",
      "Preference at .75 Avg Reward: -25.699487103558965\n",
      "Preference at .90 Avg Reward: 168.53780060602693\n"
     ]
    }
   ],
   "source": [
    "baseline = get_average_reward('baseline')\n",
    "rand_every_1 = get_average_reward('random', freq=1)\n",
    "rand_every_10 = get_average_reward('random', freq=10)\n",
    "worst_every_1 = get_average_reward('adversarial', freq=1)\n",
    "worst_every_10 = get_average_reward('adversarial', freq=10)\n",
    "preference_50 = get_average_reward('preference', threshold=0.50)\n",
    "preference_75 = get_average_reward('preference', threshold=0.75)\n",
    "preference_90 = get_average_reward('preference', threshold=0.9)\n",
    "\n",
    "print(f\"Baseline Avg Reward: {baseline}\")\n",
    "print(f\"Random Action Every 1 Avg Reward: {rand_every_1}\")\n",
    "print(f\"Random Action Every 10 Avg Reward: {rand_every_10}\")\n",
    "print(f\"Worst Action Every 1 Avg Reward: {worst_every_1}\")\n",
    "print(f\"Worst Action Every 10 Avg Reward: {worst_every_10}\")\n",
    "print(f\"Preference at .50 Avg Reward: {preference_50}\")\n",
    "print(f\"Preference at .75 Avg Reward: {preference_75}\")\n",
    "print(f\"Preference at .90 Avg Reward: {preference_90}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
