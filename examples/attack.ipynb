{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Attack\n",
    "\n",
    "This notebook looks into how ARLIN can be used to create more effective adversarial attacks. The notebook will show the average reward gained and total number of attacks in various attack scenarios against the same trained RL model:\n",
    "\n",
    "- Random action every step\n",
    "- Worst-case action every step\n",
    "- Worst-case action every 10 steps\n",
    "- Least-preferred action based on threshold (https://arxiv.org/pdf/1703.06748.pdf)\n",
    "- ARLIN-informed actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import arlin.dataset.loaders as loaders\n",
    "from arlin.dataset import XRLDataset\n",
    "from arlin.dataset.collectors import SB3PPODataCollector, SB3PPODatapoint\n",
    "\n",
    "from arlin.generation import generate_clusters, generate_embeddings\n",
    "import arlin.analysis.visualization as viz\n",
    "from arlin.analysis import ClusterAnalyzer, LatentAnalyzer\n",
    "from arlin.samdp import SAMDP\n",
    "import arlin.utils.saving_loading as sl_utils\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model sb3/ppo-LunarLander-v2/ppo-LunarLander-v2.zip from huggingface...\n",
      "INFO:root:Loading ppo model ppo-LunarLander-v2.zip with stable_baselines3...\n",
      "INFO:root:Loading ppo model adv_ppo_lunar.zip with stable_baselines3...\n",
      "INFO:root:Loading data from ./data/LunarLander-50000-Embeddings.npy...\n",
      "INFO:root:\tData loaded successfully.\n",
      "INFO:root:Loading data from ./data/LunarLander-50000-Clusters.npy...\n",
      "INFO:root:\tData loaded successfully.\n",
      "INFO:root:Loading data from ./models/cluster_algos.npy...\n",
      "INFO:root:\tData loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "load_data = True\n",
    "load_embeddings = True\n",
    "load_clusters = True\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode='rgb_array')\n",
    "\n",
    "# Load the SB3 model from Huggingface\n",
    "model = loaders.load_hf_sb_model(repo_id=\"sb3/ppo-LunarLander-v2\",\n",
    "                                 filename=\"ppo-LunarLander-v2.zip\",\n",
    "                                 algo_str=\"ppo\")\n",
    "\n",
    "adv_model = loaders.load_sb_model('./models/adv_ppo_lunar.zip', 'ppo')\n",
    "\n",
    "# Create the datapoint collector for SB3 PPO Datapoints with the model's policy\n",
    "collector = SB3PPODataCollector(datapoint_cls=SB3PPODatapoint,\n",
    "                                policy=model.policy)\n",
    "\n",
    "# Instantiate the XRL Dataset\n",
    "dataset = XRLDataset(env, collector=collector)\n",
    "\n",
    "if load_data:\n",
    "    # Load the dataset, embeddings, and clusters\n",
    "    dataset.load('./data/LunarLander-50000.npz')\n",
    "else:\n",
    "    dataset.fill(num_datapoints=50000)\n",
    "    dataset.save(file_path='./data/LunarLander-50000.npz')\n",
    "\n",
    "if load_embeddings:\n",
    "    embeddings = sl_utils.load_data(file_path='./data/LunarLander-50000-Embeddings.npy')\n",
    "else:\n",
    "    embeddings = generate_embeddings(dataset=dataset,\n",
    "                                activation_key='latent_actors',\n",
    "                                perplexity=500,\n",
    "                                n_train_iter=2000,\n",
    "                                output_dim=2,\n",
    "                                seed=12345)\n",
    "    sl_utils.save_data(embeddings, './data/LunarLander-50000-Embeddings.npy')\n",
    "\n",
    "if load_clusters:\n",
    "    clusters = sl_utils.load_data(file_path='./data/LunarLander-50000-Clusters.npy')\n",
    "    [start_algo, term_algo, mid_algo] = sl_utils.load_data(file_path='./models/cluster_algos.npy', allow_pickle=True)\n",
    "else:\n",
    "    clusters, start_algo, term_algo, mid_algo = generate_clusters(dataset=dataset,\n",
    "                                     num_clusters=30)\n",
    "\n",
    "    sl_utils.save_data(clusters, './data/LunarLander-50000-Clusters.npy')\n",
    "    sl_utils.save_data(data=[start_algo, term_algo, mid_algo], file_path='./models/cluster_algos.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARLIN Usage\n",
    "\n",
    "Let's use the ARLIN toolkit to identify when we should be performing our adversarial\n",
    "attack, and which actions we should target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_latent_analytics(embeddings: np.ndarray, \n",
    "                           clusters: np.ndarray, \n",
    "                           dataset: XRLDataset):\n",
    "    \"\"\"Graph visualizations of different latent space analytics over embeddings.\"\"\"\n",
    "    \n",
    "    # Create a grapher to generate data used for analysis.\n",
    "    grapher = LatentAnalyzer(embeddings, dataset)\n",
    "    \n",
    "    # Clusters\n",
    "    cluster_data = grapher.clusters_graph_data(clusters)\n",
    "    # Episode progression\n",
    "    ep_prog_data = grapher.episode_prog_graph_data()\n",
    "    # Greedy action confidence\n",
    "    conf_data = grapher.confidence_data()\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", \"latent_analytics\")\n",
    "    \n",
    "    # Graph multiple analytics as subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path,\n",
    "                                           figure_title='Latent Analytics', \n",
    "                                           graph_datas=[conf_data, \n",
    "                                                        cluster_data, \n",
    "                                                        ep_prog_data])\n",
    "\n",
    "def graph_cluster_analytics(dataset, clusters):\n",
    "    \"\"\"Graph analytics for each cluster\"\"\"\n",
    "    \n",
    "    # Create grapher to graph cluster analytics\n",
    "    grapher = ClusterAnalyzer(dataset, clusters)\n",
    "    \n",
    "    # grapher.cluster_state_analysis(7,\n",
    "    #                                env, \n",
    "    #                                os.path.join(\".\", \"outputs\", \"attack\", \"cluster_state_analysis\"))\n",
    "    \n",
    "    # grapher.cluster_state_analysis(10,\n",
    "    #                                env, \n",
    "    #                                os.path.join(\".\", \"outputs\", \"attack\", \"cluster_state_analysis\"))\n",
    "    \n",
    "    # grapher.cluster_state_analysis(4,\n",
    "    #                                env, \n",
    "    #                                os.path.join(\".\", \"outputs\", \"attack\", \"cluster_state_analysis\"))\n",
    "    \n",
    "    # grapher.cluster_state_analysis(17,\n",
    "    #                                env, \n",
    "    #                                os.path.join(\".\", \"outputs\", \"attack\", \"cluster_state_analysis\"))\n",
    "    \n",
    "    # Mean confidence per cluster\n",
    "    cluster_conf = grapher.cluster_confidence()\n",
    "    # Mean total reward per cluster\n",
    "    cluster_rewards = grapher.cluster_rewards()\n",
    "    # Mean value per cluster\n",
    "    cluster_values = grapher.cluster_values()\n",
    "    \n",
    "    # Graph individual graphs per data\n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", 'cluster_analytics')\n",
    "    \n",
    "    # Graph multiple subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path, \n",
    "                                           figure_title='Cluster Analytics', \n",
    "                                           graph_datas=[cluster_conf,\n",
    "                                                        cluster_values,\n",
    "                                                        cluster_rewards])\n",
    "\n",
    "def samdp(clusters: np.ndarray,\n",
    "          dataset: XRLDataset):\n",
    "    \"\"\"Generate a semi-aggregated Markov decision process.\"\"\"\n",
    "    \n",
    "    # Create the SAMDP\n",
    "    samdp = SAMDP(clusters, dataset)\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", 'samdp')\n",
    "    \n",
    "    # Simplified graph with all possible connections (regardless of action taken)\n",
    "    # simplified_graph = samdp.save_simplified_graph(f'{base_path}/samdp_simplified.png')\n",
    "    \n",
    "    # path_path = os.path.join(base_path, f\"samdp_path_15_18\")\n",
    "    \n",
    "    # # Path from cluster 15 to cluster 18\n",
    "    # # Only the most likely path is shown\n",
    "    # samdp.save_paths(15, \n",
    "    #                  18,  \n",
    "    #                  f'{os.path.join(base_path, f\"samdp_path_15_18\")}_bp.png', \n",
    "    #                  best_path_only=True)\n",
    "    \n",
    "    # Path from cluster 9 to cluster 18\n",
    "    # Only the most likely path is shown\n",
    "    # samdp.save_paths(13, \n",
    "    #                  18,  \n",
    "    #                  f'{os.path.join(base_path, f\"samdp_path_13_18\")}_bp.png', \n",
    "    #                  best_path_only=True)\n",
    "    \n",
    "    # # Path from cluster 9 to cluster 18\n",
    "    # # Only the most likely path is shown\n",
    "    # samdp.save_paths(10, \n",
    "    #                  18,  \n",
    "    #                  f'{os.path.join(base_path, f\"samdp_path_10_18\")}_bp.png', \n",
    "    #                  best_path_only=True)\n",
    "    \n",
    "    # Path from cluster 9 to cluster 18\n",
    "    # Only the most likely path is shown\n",
    "    samdp.save_paths(1, \n",
    "                     34,  \n",
    "                     f'{os.path.join(base_path, f\"samdp_path_1_34\")}_bp.png', \n",
    "                     best_path_only=True)\n",
    "    \n",
    "    samdp.save_paths(1, \n",
    "                     33,  \n",
    "                     f'{os.path.join(base_path, f\"samdp_path_1_33\")}_bp.png', \n",
    "                     best_path_only=True)\n",
    "\n",
    "    samdp.save_paths(9, \n",
    "                     34,  \n",
    "                     f'{os.path.join(base_path, f\"samdp_path_9_34\")}_bp.png', \n",
    "                     best_path_only=True)\n",
    "    \n",
    "    samdp.save_paths(9, \n",
    "                     33,  \n",
    "                     f'{os.path.join(base_path, f\"samdp_path_9_33\")}_bp.png', \n",
    "                     best_path_only=True)\n",
    "    \n",
    "    # Show all paths that lead to cluster 18\n",
    "    # # Action into cluster 18 shown, rest is simplified\n",
    "    # samdp.save_all_paths_to(24, \n",
    "    #                         os.path.join(base_path, f\"samdp_paths_to_24.png\"))\n",
    "    \n",
    "    # # Show all paths that lead to cluster 18\n",
    "    # # Action into cluster 18 shown, rest is simplified\n",
    "    # samdp.save_all_paths_to(23, \n",
    "    #                         os.path.join(base_path, f\"samdp_paths_to_23.png\"))\n",
    "    \n",
    "    # # Show all paths that lead to cluster 18\n",
    "    # # Action into cluster 18 shown, rest is simplified\n",
    "    # samdp.save_all_paths_to(22, \n",
    "    #                         os.path.join(base_path, f\"samdp_paths_to_22.png\"))\n",
    "    \n",
    "    # samdp.save_all_paths_to(34, \n",
    "    #                         os.path.join(base_path, f\"samdp_paths_to_34.png\"))\n",
    "    \n",
    "    # samdp.save_txt('./outputs/attack/samdp/text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Generating SAMDP.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Generating SAMDP Graph.\n",
      "INFO:root:Finding paths from Cluster 1 to Cluster 34...\n",
      "INFO:root:Highest probability of getting from Cluster 1 to Cluster 34:\n",
      "INFO:root:\tvia Action 0: 0.41%\n",
      "INFO:root:\t\tCluster 1 to Cluster 9 with 76.64%\n",
      "INFO:root:\t\tCluster 9 to Cluster 34 with 0.53%\n",
      "INFO:root:\tvia Action 1: 0.41%\n",
      "INFO:root:\t\tCluster 1 to Cluster 9 with 76.67%\n",
      "INFO:root:\t\tCluster 9 to Cluster 34 with 0.53%\n",
      "INFO:root:\tvia Action 2: 0.32%\n",
      "INFO:root:\t\tCluster 1 to Cluster 9 with 60.0%\n",
      "INFO:root:\t\tCluster 9 to Cluster 34 with 0.53%\n",
      "INFO:root:\tvia Action 3: 0.37%\n",
      "INFO:root:\t\tCluster 1 to Cluster 9 with 69.57%\n",
      "INFO:root:\t\tCluster 9 to Cluster 34 with 0.53%\n",
      "INFO:root:\tBest Option: Action 1 with 0.41%\n",
      "INFO:root:\tBest Path:\n",
      "INFO:root:\t\tCluster 1 to Cluster 9 with 76.67%\n",
      "INFO:root:\t\tCluster 9 to Cluster 34 with 0.53%\n",
      "INFO:root:Saving SAMDP path from Cluster 1 to Cluster 34 png to ./outputs/attack/samdp/samdp_path_1_34_bp.png...\n",
      "INFO:root:Finding paths from Cluster 1 to Cluster 33...\n",
      "INFO:root:Highest probability of getting from Cluster 1 to Cluster 33:\n",
      "INFO:root:\tvia Action 0: 15.49%\n",
      "INFO:root:\t\tCluster 1 to Cluster 9 with 76.64%\n",
      "INFO:root:\t\tCluster 9 to Cluster 33 with 20.21%\n",
      "INFO:root:\tvia Action 1: 15.5%\n",
      "INFO:root:\t\tCluster 1 to Cluster 9 with 76.67%\n",
      "INFO:root:\t\tCluster 9 to Cluster 33 with 20.21%\n",
      "INFO:root:\tvia Action 2: 12.13%\n",
      "INFO:root:\t\tCluster 1 to Cluster 9 with 60.0%\n",
      "INFO:root:\t\tCluster 9 to Cluster 33 with 20.21%\n",
      "INFO:root:\tvia Action 3: 14.06%\n",
      "INFO:root:\t\tCluster 1 to Cluster 9 with 69.57%\n",
      "INFO:root:\t\tCluster 9 to Cluster 33 with 20.21%\n",
      "INFO:root:\tBest Option: Action 1 with 15.5%\n",
      "INFO:root:\tBest Path:\n",
      "INFO:root:\t\tCluster 1 to Cluster 9 with 76.67%\n",
      "INFO:root:\t\tCluster 9 to Cluster 33 with 20.21%\n",
      "INFO:root:Saving SAMDP path from Cluster 1 to Cluster 33 png to ./outputs/attack/samdp/samdp_path_1_33_bp.png...\n",
      "INFO:root:Finding paths from Cluster 9 to Cluster 34...\n",
      "INFO:root:Highest probability of getting from Cluster 9 to Cluster 34:\n",
      "INFO:root:\tvia Action 1: 25.0%\n",
      "INFO:root:\t\tCluster 9 to Cluster 34 with 25.0%\n",
      "INFO:root:\tBest Option: Action 1 with 25.0%\n",
      "INFO:root:\tBest Path:\n",
      "INFO:root:\t\tCluster 9 to Cluster 34 with 25.0%\n",
      "INFO:root:Saving SAMDP path from Cluster 9 to Cluster 34 png to ./outputs/attack/samdp/samdp_path_9_34_bp.png...\n",
      "INFO:root:Finding paths from Cluster 9 to Cluster 33...\n",
      "INFO:root:Highest probability of getting from Cluster 9 to Cluster 33:\n",
      "INFO:root:\tvia Action 0: 22.62%\n",
      "INFO:root:\t\tCluster 9 to Cluster 33 with 22.62%\n",
      "INFO:root:\tvia Action 1: 1.48%\n",
      "INFO:root:\t\tCluster 9 to Cluster 21 with 50.0%\n",
      "INFO:root:\t\tCluster 21 to Cluster 1 with 62.03%\n",
      "INFO:root:\t\tCluster 1 to Cluster 33 with 4.79%\n",
      "INFO:root:\tvia Action 2: 4.79%\n",
      "INFO:root:\t\tCluster 9 to Cluster 1 with 100.0%\n",
      "INFO:root:\t\tCluster 1 to Cluster 33 with 4.79%\n",
      "INFO:root:\tvia Action 3: 4.79%\n",
      "INFO:root:\t\tCluster 9 to Cluster 1 with 100.0%\n",
      "INFO:root:\t\tCluster 1 to Cluster 33 with 4.79%\n",
      "INFO:root:\tBest Option: Action 0 with 22.62%\n",
      "INFO:root:\tBest Path:\n",
      "INFO:root:\t\tCluster 9 to Cluster 33 with 22.62%\n",
      "INFO:root:Saving SAMDP path from Cluster 9 to Cluster 33 png to ./outputs/attack/samdp/samdp_path_9_33_bp.png...\n"
     ]
    }
   ],
   "source": [
    "# graph_latent_analytics(embeddings, clusters, dataset)\n",
    "# graph_cluster_analytics(dataset, clusters)\n",
    "samdp(clusters, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_attack(model_type: str, \n",
    "                  timestep: int, \n",
    "                  freq: int = 0,\n",
    "                  preference: float = 0, \n",
    "                  threshold: float = 1.0) -> bool:\n",
    "    \"\"\"Check whether or not we should attack at the given timestep.\n",
    "\n",
    "    Args:\n",
    "        model_type (str): Type of model we want to run.\n",
    "        timestep (int): Current timestep\n",
    "        freq (int, optional): Frequency of attack. Defaults to 0.\n",
    "        preference (float, optional): Delta between most and least preferred action.\n",
    "            Defaults to 0.\n",
    "        threshold (float, optional): Threshold for preference attack. Defaults to 1.0.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If invalid model type is given.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether or not to attack\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_type == 'baseline':\n",
    "        return False\n",
    "    elif model_type == 'random' or model_type == 'adversarial':\n",
    "        if timestep % freq == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif model_type == 'preference':\n",
    "        if preference > threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model_type {model_type} given.\")\n",
    "\n",
    "def get_action(obs: np.ndarray,\n",
    "               model_type: str, \n",
    "               timestep: int, \n",
    "               freq: int = 0,\n",
    "               preference: float = 0, \n",
    "               threshold: float = 1.0) -> int:\n",
    "    \"\"\"Get the action to take at the given timestep.\n",
    "\n",
    "    Args:\n",
    "        obs (np.ndarray): Current observation from the agent.\n",
    "        model_type (str): Type of model we want to run.\n",
    "        timestep (int): Current timestep\n",
    "        freq (int, optional): Frequency of attack. Defaults to 0.\n",
    "        preference (float, optional): Delta between most and least preferred action.\n",
    "            Defaults to 0.\n",
    "        threshold (float, optional): Threshold for preference attack. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        int: Action value to take.\n",
    "    \"\"\"\n",
    "    adv = False\n",
    "    if should_attack(model_type, timestep, freq, preference, threshold):\n",
    "        adv = True\n",
    "        if model_type == 'random':\n",
    "            rng = np.random.default_rng(12345)\n",
    "            action = rng.integers(low=0, high=env.action_space.n, size=1).item()\n",
    "        else:\n",
    "            action, _ = adv_model.predict(obs, deterministic=True)\n",
    "    else:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "    return action, adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_reward(model_type: str ='baseline', \n",
    "                       freq: int = 0,\n",
    "                       threshold: int = 1) -> float:\n",
    "    \"\"\"Average reward over 10 episodes while the model is being attacked.\n",
    "    \n",
    "    Attacks happen at the given freq and come from the given model type. \n",
    "        - Baseline does not include any adversarial attacks.\n",
    "        - Random chooses the action randomly.\n",
    "        - Adversarial chooses the worst possible action at that point in time.\n",
    "        - Preference chooses the least preferred action when the pref is above a threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_attacks = []\n",
    "    \n",
    "    dir_name = os.path.join(\"./outputs/attack/gifs/\", model_type)\n",
    "    \n",
    "    if freq != 0:\n",
    "        dir_name = dir_name + f\"-{freq}_freq\"\n",
    "    \n",
    "    if threshold != 1:\n",
    "        dir_name = dir_name + f\"-{threshold}_thresh\"\n",
    "    \n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    gif_lists = []\n",
    "    \n",
    "    for ep in range(10):\n",
    "        obs, _ = env.reset(seed=1234 + ep)\n",
    "        images = [Image.fromarray(env.render())]\n",
    "        done = False\n",
    "        step = 0\n",
    "        ep_rew = 0\n",
    "        adv_attacks = 0\n",
    "        \n",
    "        while not done:\n",
    "            internal_data, _ = collector.collect_internal_data(obs)\n",
    "            probs = internal_data.dist_probs\n",
    "            preference = probs.max() - probs.min()\n",
    "            \n",
    "            action, adv = get_action(obs, model_type, step, freq, preference, threshold)\n",
    "            if adv:\n",
    "                adv_attacks += 1\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            images.append(Image.fromarray(env.render()))\n",
    "            ep_rew += reward\n",
    "            done = terminated or truncated\n",
    "            step += 1\n",
    "        \n",
    "        gif_lists.append(images)\n",
    "        episode_rewards.append(ep_rew)\n",
    "        episode_attacks.append(adv_attacks)\n",
    "    \n",
    "    idx = episode_rewards.index(max(episode_rewards))\n",
    "    save_path = os.path.join(dir_name, f'episode_{idx}-max.gif')\n",
    "    gif_lists[idx][0].save(save_path, save_all=True, append_images=gif_lists[idx], duration=30)\n",
    "    \n",
    "    idx = episode_rewards.index(min(episode_rewards))\n",
    "\n",
    "    save_path = os.path.join(dir_name, f'episode_{idx}-min.gif')\n",
    "    gif_lists[idx][0].save(save_path, save_all=True, append_images=gif_lists[idx], duration=30)\n",
    "    \n",
    "    return sum(episode_rewards) / 10, sum(episode_attacks) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Avg Reward: 247.46209077695022 with 0.0 attacks\n",
      "Random Action Every 1 Avg Reward: -588.4916522579238 with 95.7 attacks\n",
      "Random Action Every 10 Avg Reward: 138.2434657663849 with 100.0 attacks\n",
      "Worst Action Every 1 Avg Reward: -594.7959537977795 with 63.0 attacks\n",
      "Worst Action Every 10 Avg Reward: 139.18397864543132 with 100.0 attacks\n",
      "Preference at .50 Avg Reward: -547.7386702780462 with 59.0 attacks\n",
      "Preference at .75 Avg Reward: -25.699487103558965 with 18.4 attacks\n",
      "Preference at .90 Avg Reward: 168.53780060602693 with 5.8 attacks\n"
     ]
    }
   ],
   "source": [
    "baseline, base_n_attack = get_average_reward('baseline')\n",
    "rand_every_1, rand1_n_attack = get_average_reward('random', freq=1)\n",
    "rand_every_10, rand10_n_attack = get_average_reward('random', freq=10)\n",
    "worst_every_1, worst1_n_attack = get_average_reward('adversarial', freq=1)\n",
    "worst_every_10, worst10_n_attack = get_average_reward('adversarial', freq=10)\n",
    "preference_50, pref50_n_attack = get_average_reward('preference', threshold=0.50)\n",
    "preference_75, pref75_n_attack = get_average_reward('preference', threshold=0.75)\n",
    "preference_90, pref90_n_attack = get_average_reward('preference', threshold=0.9)\n",
    "\n",
    "print(f\"Baseline Avg Reward: {baseline} with {base_n_attack} attacks\")\n",
    "print(f\"Random Action Every 1 Avg Reward: {rand_every_1} with {rand1_n_attack} attacks\")\n",
    "print(f\"Random Action Every 10 Avg Reward: {rand_every_10} with {rand10_n_attack} attacks\")\n",
    "print(f\"Worst Action Every 1 Avg Reward: {worst_every_1} with {worst1_n_attack} attacks\")\n",
    "print(f\"Worst Action Every 10 Avg Reward: {worst_every_10} with {worst10_n_attack} attacks\")\n",
    "print(f\"Preference at .50 Avg Reward: {preference_50} with {pref50_n_attack} attacks\")\n",
    "print(f\"Preference at .75 Avg Reward: {preference_75} with {pref75_n_attack} attacks\")\n",
    "print(f\"Preference at .90 Avg Reward: {preference_90} with {pref90_n_attack} attacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal: [32] 268.6094182400691 65\n",
      "Terminal: [32] 235.27491673455904 41\n",
      "Terminal: [32] 223.95616732714305 42\n",
      "Terminal: [32] 273.6596252490928 43\n",
      "Terminal: [32] 223.4218360915082 49\n",
      "Terminal: [33] 30.233268838799972 15\n",
      "Terminal: [32] 280.1934542704997 58\n",
      "Terminal: [32] 200.99772107592568 378\n",
      "Terminal: [32] 277.72075190658995 31\n",
      "Terminal: [32] 252.6146954932242 306\n",
      "Final: 226.66818552274117 with 102.8 attacks\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "np.random.seed(1234)\n",
    "episodes = []\n",
    "attacks = []\n",
    "gif_lists = []\n",
    "\n",
    "dir_name = \"./outputs/attack/gifs/arlin/\"\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "for ep in range(10):\n",
    "    obs, _ = env.reset(seed=1234 + ep)\n",
    "    images = [Image.fromarray(env.render())]\n",
    "    \n",
    "    done = False\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    last_pred = -1\n",
    "    n_attacks = 0\n",
    "    \n",
    "    while not done:\n",
    "        internal_data, _ = collector.collect_internal_data(obs)\n",
    "        probs = internal_data.dist_probs\n",
    "        preference = probs.max() - probs.min()\n",
    "        \n",
    "        if step == 0:\n",
    "            prediction = start_algo.predict(np.array([internal_data.critic_values]).reshape(-1, 1))\n",
    "            if last_pred != prediction + 30:\n",
    "                # print(f'Initial: {prediction + 14}')\n",
    "                last_pred = prediction\n",
    "        else:\n",
    "            latent = internal_data.latent_actors\n",
    "            value = internal_data.critic_values\n",
    "            confidence = np.amax(internal_data.dist_probs)\n",
    "            \n",
    "            data = np.concatenate([latent,\n",
    "                                # np.expand_dims(action, axis=-1),\n",
    "                                np.expand_dims(value, axis=-1),\n",
    "                                np.expand_dims(reward, axis=-1),\n",
    "                                # np.expand_dims(total_reward, axis=-1),\n",
    "                                np.expand_dims(confidence, axis=-1)], axis=-1)\n",
    "            prediction = mid_algo.predict(data.reshape(1, -1))\n",
    "            if last_pred != prediction:\n",
    "                # print(f'Mid: {prediction}')\n",
    "                last_pred = prediction\n",
    "\n",
    "        if prediction == 9:\n",
    "            action = 0\n",
    "            n_attacks += 1\n",
    "        elif prediction == 1:\n",
    "            action = 1\n",
    "            n_attacks += 1\n",
    "        else:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        images.append(Image.fromarray(env.render()))\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        step += 1\n",
    "\n",
    "    gif_lists.append(images)\n",
    "    prediction = start_algo.predict(np.array([total_reward]).reshape(-1, 1))\n",
    "    print(f'Terminal: {prediction + 30 + 2} {total_reward} {n_attacks}')\n",
    "    # render = env.render()\n",
    "    # im = Image.fromarray(render)\n",
    "    # im.save(f'./outputs/attack/images/{ep}_{prediction + 20}.png')\n",
    "    episodes.append(total_reward)\n",
    "    attacks.append(n_attacks)\n",
    "\n",
    "idx = episodes.index(max(episodes))\n",
    "save_path = os.path.join(dir_name, f'episode_{idx}-max.gif')\n",
    "gif_lists[idx][0].save(save_path, save_all=True, append_images=gif_lists[idx], duration=30)\n",
    "\n",
    "idx = episodes.index(min(episodes))\n",
    "save_path = os.path.join(dir_name, f'episode_{idx}-min.gif')\n",
    "gif_lists[idx][0].save(save_path, save_all=True, append_images=gif_lists[idx], duration=30)\n",
    "\n",
    "print(f'Final: {sum(episodes) / 10} with {sum(attacks) / 10} attacks')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
