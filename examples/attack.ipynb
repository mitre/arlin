{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Attack\n",
    "\n",
    "This notebook looks into how ARLIN can be used to create more effective adversarial attacks. The notebook will show the average reward gained and total number of attacks in various attack scenarios against the same trained RL model:\n",
    "\n",
    "- Random action every step\n",
    "- Worst-case action every step\n",
    "- Worst-case action every 10 steps\n",
    "- Least-preferred action based on threshold (https://arxiv.org/pdf/1703.06748.pdf)\n",
    "- ARLIN-informed actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import arlin.dataset.loaders as loaders\n",
    "from arlin.dataset import XRLDataset\n",
    "from arlin.dataset.collectors import SB3PPODataCollector\n",
    "from arlin.dataset.collectors.datapoints import SB3PPODatapoint\n",
    "\n",
    "from arlin.generation import generate_clusters, generate_embeddings\n",
    "import arlin.analysis.visualization as viz\n",
    "from arlin.analysis import ClusterAnalyzer, LatentAnalyzer\n",
    "from arlin.samdp import SAMDP\n",
    "import arlin.utils.saving_loading as sl_utils\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = True\n",
    "load_embeddings = True\n",
    "load_clusters = True\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode='rgb_array')\n",
    "\n",
    "# Load the SB3 model from Huggingface\n",
    "model = loaders.load_hf_sb_model(repo_id=\"sb3/ppo-LunarLander-v2\",\n",
    "                                 filename=\"ppo-LunarLander-v2.zip\",\n",
    "                                 algo_str=\"ppo\")\n",
    "\n",
    "adv_model = loaders.load_sb_model('./models/adv_ppo_lunar.zip', 'ppo')\n",
    "\n",
    "# Create the datapoint collector for SB3 PPO Datapoints with the model's policy\n",
    "collector = SB3PPODataCollector(datapoint_cls=SB3PPODatapoint,\n",
    "                                policy=model.policy)\n",
    "\n",
    "# Instantiate the XRL Dataset\n",
    "dataset = XRLDataset(env, collector=collector)\n",
    "\n",
    "if load_data:\n",
    "    # Load the dataset, embeddings, and clusters\n",
    "    dataset.load('./data/LunarLander-50000.npz')\n",
    "else:\n",
    "    dataset.fill(num_datapoints=50000, randomness=0.2)\n",
    "    dataset.save(file_path='./data/LunarLander-50000.npz')\n",
    "\n",
    "if load_embeddings:\n",
    "    embeddings = sl_utils.load_data(file_path='./data/LunarLander-50000-Embeddings.npy')\n",
    "else:\n",
    "    embeddings = generate_embeddings(dataset=dataset,\n",
    "                                activation_key='latent_actors',\n",
    "                                perplexity=500,\n",
    "                                n_train_iter=1500,\n",
    "                                output_dim=2,\n",
    "                                seed=12345)\n",
    "    sl_utils.save_data(embeddings, './data/LunarLander-50000-Embeddings.npy')\n",
    "\n",
    "if load_clusters:\n",
    "    clusters = sl_utils.load_data(file_path='./data/LunarLander-50000-Clusters.npy')\n",
    "    [start_algo, mid_algo, term_algo] = sl_utils.load_data(file_path='./models/cluster_algos.npy', allow_pickle=True)\n",
    "else:\n",
    "    clusters, start_algo, mid_algo, term_algo = generate_clusters(\n",
    "        dataset,\n",
    "        [\"latent_actors\", \"critic_values\"],\n",
    "        [\"latent_actors\", \"critic_values\"],\n",
    "        [\"latent_actors\", \"critic_values\", \"rewards\"],\n",
    "        20,\n",
    "        seed=1234\n",
    "        )\n",
    "\n",
    "    sl_utils.save_data(clusters, './data/LunarLander-50000-Clusters.npy')\n",
    "    sl_utils.save_data(data=[start_algo, mid_algo, term_algo], file_path='./models/cluster_algos.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARLIN Usage\n",
    "\n",
    "Let's use the ARLIN toolkit to identify when we should be performing our adversarial\n",
    "attack, and which actions we should target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_latent_analytics(embeddings: np.ndarray, \n",
    "                           clusters: np.ndarray, \n",
    "                           dataset: XRLDataset):\n",
    "    \"\"\"Graph visualizations of different latent space analytics over embeddings.\"\"\"\n",
    "    \n",
    "    # Create a grapher to generate data used for analysis.\n",
    "    grapher = LatentAnalyzer(embeddings, dataset)\n",
    "    \n",
    "    embeddings_data = grapher.embeddings_graph_data()\n",
    "    # Clusters\n",
    "    cluster_data = grapher.clusters_graph_data(clusters)\n",
    "    \n",
    "    decision_boundaries = grapher.decision_boundary_graph_data()\n",
    "    # Episode progression\n",
    "    ep_prog_data = grapher.episode_prog_graph_data()\n",
    "    # Greedy action confidence\n",
    "    conf_data = grapher.confidence_data()\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", \"latent_analytics\")\n",
    "    \n",
    "    # Graph multiple analytics as subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics-total.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path,\n",
    "                                           figure_title='Latent Analytics', \n",
    "                                           graph_datas=[ep_prog_data, \n",
    "                                                        conf_data, \n",
    "                                                        decision_boundaries],\n",
    "                                           horizontal=False)\n",
    "    combined_path_2 = os.path.join(base_path, 'combined_analytics-generate.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path_2,\n",
    "                                           figure_title='Latent Analytics', \n",
    "                                           graph_datas=[embeddings_data, \n",
    "                                                        cluster_data],\n",
    "                                           horizontal=False)\n",
    "\n",
    "def graph_cluster_analytics(dataset, clusters):\n",
    "    \"\"\"Graph analytics for each cluster\"\"\"\n",
    "    \n",
    "    # Create grapher to graph cluster analytics\n",
    "    grapher = ClusterAnalyzer(dataset, clusters)\n",
    "    \n",
    "    for i in range(22, 25):\n",
    "        grapher.cluster_state_analysis(i,\n",
    "                                       env,\n",
    "                                       os.path.join(\".\", \"outputs\", \"attack\", \"cluster_state_analysis\"))\n",
    "\n",
    "    # grapher.cluster_state_analysis(9,\n",
    "    #                                env, \n",
    "    #                                os.path.join(\".\", \"outputs\", \"attack\", \"cluster_state_analysis\"))\n",
    "    \n",
    "    # Mean confidence per cluster\n",
    "    cluster_conf = grapher.cluster_confidence()\n",
    "    # Mean total reward per cluster\n",
    "    cluster_rewards = grapher.cluster_rewards()\n",
    "    # Mean value per cluster\n",
    "    cluster_values = grapher.cluster_values()\n",
    "    \n",
    "    # Graph individual graphs per data\n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", 'cluster_analytics')\n",
    "    \n",
    "    # Graph multiple subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path, \n",
    "                                           figure_title='Cluster Analytics', \n",
    "                                           graph_datas=[cluster_conf,\n",
    "                                                        cluster_values,\n",
    "                                                        cluster_rewards],\n",
    "                                           horizontal=False)\n",
    "\n",
    "def samdp(clusters: np.ndarray,\n",
    "          dataset: XRLDataset):\n",
    "    \"\"\"Generate a semi-aggregated Markov decision process.\"\"\"\n",
    "    \n",
    "    # Create the SAMDP\n",
    "    samdp = SAMDP(clusters, dataset)\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", 'samdp')\n",
    "    \n",
    "    # Simplified graph with all possible connections (regardless of action taken)\n",
    "    simplified_graph = samdp.save_simplified_graph(f'{base_path}/samdp_simplified.png')\n",
    "    \n",
    "    samdp.save_terminal_paths(f'{os.path.join(base_path, f\"samdp_terminals_23\")}.png', \n",
    "                              best_path=True,\n",
    "                              term_cluster_id=23)\n",
    "    \n",
    "    samdp.save_txt('./outputs/attack/samdp/text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_latent_analytics(embeddings, clusters, dataset)\n",
    "# graph_cluster_analytics(dataset, clusters)\n",
    "samdp(clusters, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_attack(model_type: str, \n",
    "                  timestep: int, \n",
    "                  freq: int = 0,\n",
    "                  preference: float = 0, \n",
    "                  threshold: float = 1.0) -> bool:\n",
    "    \"\"\"Check whether or not we should attack at the given timestep.\n",
    "\n",
    "    Args:\n",
    "        model_type (str): Type of model we want to run.\n",
    "        timestep (int): Current timestep\n",
    "        freq (int, optional): Frequency of attack. Defaults to 0.\n",
    "        preference (float, optional): Delta between most and least preferred action.\n",
    "            Defaults to 0.\n",
    "        threshold (float, optional): Threshold for preference attack. Defaults to 1.0.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If invalid model type is given.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether or not to attack\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_type == 'baseline':\n",
    "        return False\n",
    "    elif model_type == 'random' or model_type == 'adversarial':\n",
    "        if timestep % freq == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif model_type == 'preference':\n",
    "        if preference > threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model_type {model_type} given.\")\n",
    "\n",
    "def get_action(obs: np.ndarray,\n",
    "               model_type: str, \n",
    "               timestep: int, \n",
    "               freq: int = 0,\n",
    "               preference: float = 0, \n",
    "               threshold: float = 1.0) -> Tuple[int, bool]:\n",
    "    \"\"\"Get the action to take at the given timestep.\n",
    "\n",
    "    Args:\n",
    "        obs (np.ndarray): Current observation from the agent.\n",
    "        model_type (str): Type of model we want to run.\n",
    "        timestep (int): Current timestep\n",
    "        freq (int, optional): Frequency of attack. Defaults to 0.\n",
    "        preference (float, optional): Delta between most and least preferred action.\n",
    "            Defaults to 0.\n",
    "        threshold (float, optional): Threshold for preference attack. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        int: Action value to take.\n",
    "        bool: Adversarial action or not\n",
    "    \"\"\"\n",
    "    adv = False\n",
    "    if should_attack(model_type, timestep, freq, preference, threshold):\n",
    "        adv = True\n",
    "        if model_type == 'random':\n",
    "            rng = np.random.default_rng(12345)\n",
    "            action = rng.integers(low=0, high=env.action_space.n, size=1).item()\n",
    "        else:\n",
    "            action, _ = adv_model.predict(obs, deterministic=True)\n",
    "    else:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "    return action, adv\n",
    "\n",
    "def get_action_data(obs: np.ndarray,\n",
    "               model_type: str, \n",
    "               timestep: int, \n",
    "               freq: int = 0,\n",
    "               preference: float = 0, \n",
    "               threshold: float = 1.0) -> Dict[str, Any]:\n",
    "    \"\"\"Get the action to take at the given timestep.\n",
    "\n",
    "    Args:\n",
    "        obs (np.ndarray): Current observation from the agent.\n",
    "        model_type (str): Type of model we want to run.\n",
    "        timestep (int): Current timestep\n",
    "        freq (int, optional): Frequency of attack. Defaults to 0.\n",
    "        preference (float, optional): Delta between most and least preferred action.\n",
    "            Defaults to 0.\n",
    "        threshold (float, optional): Threshold for preference attack. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Action value to take.\n",
    "    \"\"\"\n",
    "    action, adv = get_action(obs, model_type, timestep, freq, preference, threshold)\n",
    "\n",
    "    return action, adv\n",
    "\n",
    "def get_model_name(model_type: str, freq: int, threshold: int):\n",
    "    model_name = model_type.capitalize()\n",
    "    \n",
    "    if freq != 0:\n",
    "        model_name = f\"{model_name}_{freq}\"\n",
    "    \n",
    "    if threshold != 1:\n",
    "        model_name = f\"{model_name}_{threshold}\"\n",
    "    \n",
    "    return model_name\n",
    "\n",
    "def create_save_dirs(model_name: str):\n",
    "    gifs_dir_name = os.path.join(\"./outputs/attack/gifs\")\n",
    "    metrics_dir_name = os.path.join(\"./outputs/attack/metrics\")\n",
    "    os.makedirs(gifs_dir_name, exist_ok=True)\n",
    "    os.makedirs(metrics_dir_name, exist_ok=True)\n",
    "    \n",
    "    return gifs_dir_name, metrics_dir_name\n",
    "\n",
    "def split_data(internal_data: SB3PPODatapoint):\n",
    "    probs = internal_data.dist_probs\n",
    "    preference = probs.max() - probs.min()\n",
    "    \n",
    "    return preference, probs\n",
    "\n",
    "def save_gifs(dir_name: str, gif_lists: List[Image.Image], episode_rewards: List[int]):\n",
    "    idx = episode_rewards.index(max(episode_rewards))\n",
    "    save_path = os.path.join(dir_name, f'episode_{idx}-max.gif')\n",
    "    gif_lists[idx][0].save(save_path, \n",
    "                           save_all=True, \n",
    "                           append_images=gif_lists[idx], \n",
    "                           duration=30, \n",
    "                           loop=0)\n",
    "    \n",
    "    idx = episode_rewards.index(min(episode_rewards))\n",
    "\n",
    "    save_path = os.path.join(dir_name, f'episode_{idx}-min.gif')\n",
    "    gif_lists[idx][0].save(save_path, \n",
    "                           save_all=True, \n",
    "                           append_images=gif_lists[idx], \n",
    "                           duration=30, \n",
    "                           loop=0)\n",
    "\n",
    "def get_averages(num_episodes: int,\n",
    "                 episode_rewards: List[int], \n",
    "                 episode_attacks: List[int],\n",
    "                 episode_perc_attack: List[int]) -> Tuple[int, int, int]:\n",
    "    avg_reward = sum(episode_rewards) / num_episodes\n",
    "    avg_attacks = sum(episode_attacks) / num_episodes\n",
    "    avg_perc_attack = (sum(episode_perc_attack) / num_episodes) * 100\n",
    "    \n",
    "    return avg_reward, avg_attacks, avg_perc_attack\n",
    "\n",
    "def action_delta_histogram(action_deltas: List[float], title: str, dir_name: str):\n",
    "    num_bins = 20\n",
    "    mu = statistics.mean(action_deltas)\n",
    "    median = statistics.median(action_deltas)\n",
    "    sigma = statistics.stdev(action_deltas)\n",
    "    n, bins, patches = plt.hist(action_deltas, num_bins, \n",
    "                            density = 1, \n",
    "                            color ='green',\n",
    "                            alpha = 0.7)\n",
    "    \n",
    "    y = ((1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    " \n",
    "    plt.plot(bins, y, '--', color ='black')\n",
    "    plt.axvline(median, color='k', linestyle='dashed', linewidth=1)\n",
    "    _, max_ylim = plt.ylim()\n",
    "    plt.text(median*1.1, \n",
    "             max_ylim*0.9, \n",
    "             'Median: {:.2f}'.format(median))\n",
    "    \n",
    "    plt.xlabel('Delta Between GT and Adversarial Action')\n",
    "    plt.ylabel('Delta Occurence')\n",
    "    \n",
    "    plt.title(title, fontweight = \"bold\")\n",
    "    path = os.path.join(dir_name, \"action_deltas.png\")\n",
    "    plt.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def find_adversarial_dist(gt_dist: np.ndarray, adv_action: int):\n",
    "    # adv_dst = gt_dist\n",
    "    \n",
    "    print(gt_dist)\n",
    "    return\n",
    "    \n",
    "    # return adv_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_reward(model_type: str ='baseline', \n",
    "                       freq: int = 0,\n",
    "                       threshold: int = 1,\n",
    "                       num_episodes: int = 10) -> float:\n",
    "    \"\"\"Average reward over 10 episodes while the model is being attacked.\n",
    "    \n",
    "    Attacks happen at the given freq and come from the given model type. \n",
    "        - Baseline does not include any adversarial attacks.\n",
    "        - Random chooses the action randomly.\n",
    "        - Adversarial chooses the worst possible action at that point in time.\n",
    "        - Preference chooses the least preferred action when the pref is above a threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # Variables to store eval data\n",
    "    episode_rewards = []\n",
    "    episode_attacks = []\n",
    "    episode_perc_attack = []\n",
    "    episode_action_deltas = []\n",
    "    episode_obs = []\n",
    "    gif_lists = []\n",
    "    \n",
    "    model_name = get_model_name(model_type, freq, threshold)\n",
    "    gifs_dir_name, metrics_dir_name = create_save_dirs(model_name)\n",
    "    \n",
    "    # For each eval episode\n",
    "    for ep in range(num_episodes):\n",
    "        obs, _ = env.reset(seed=1234 + ep)\n",
    "        images = [Image.fromarray(env.render())]\n",
    "        ep_obs = [obs]\n",
    "        done = False\n",
    "        step = 0\n",
    "        ep_rew = 0\n",
    "        n_adv_attacks = 0\n",
    "        \n",
    "        while not done:\n",
    "            internal_data, _ = collector.collect_internal_data(obs)\n",
    "            preference, probs = split_data(internal_data)\n",
    "            \n",
    "            action, adv = get_action(obs, model_type, step, freq, preference, threshold)\n",
    "            if adv:\n",
    "                n_adv_attacks += 1\n",
    "                gt_action = np.argmax(probs).item()\n",
    "                action_delta = probs[gt_action] - probs[action]\n",
    "                episode_action_deltas.append(action_delta)\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            images.append(Image.fromarray(env.render()))\n",
    "            ep_obs.append(obs)\n",
    "            ep_rew += reward\n",
    "            done = terminated or truncated\n",
    "            step += 1\n",
    "        \n",
    "        gif_lists.append(images)\n",
    "        episode_rewards.append(ep_rew)\n",
    "        episode_attacks.append(n_adv_attacks)\n",
    "        episode_perc_attack.append(n_adv_attacks / step)\n",
    "        episode_obs.append(ep_obs)\n",
    "    \n",
    "    # save_gifs(os.path.join(gif_dir_name, model_name), gif_lists, episode_rewards)\n",
    "    avg_reward, avg_attacks, avg_perc_attack = get_averages(num_episodes,\n",
    "                                                            episode_rewards,\n",
    "                                                            episode_attacks,\n",
    "                                                            episode_perc_attack)\n",
    "    if model_type != 'baseline':\n",
    "        action_delta_histogram(episode_action_deltas, \n",
    "                               f\"Action Deltas for {model_name}\", \n",
    "                               metrics_dir_name)\n",
    "    \n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"\\tAvg Reward: {avg_reward} | Avg Num Attacks: {avg_attacks} | Avg Percent Attacks: {avg_perc_attack}\")\n",
    "    \n",
    "    return episode_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cosine_sim(baseline_obs: List[List[np.ndarray]], \n",
    "                   target_obs: List[List[List[np.ndarray]]],\n",
    "                   target_model_names: List[str],\n",
    "                   dir_name: str,\n",
    "                   num_eval: int = 1):\n",
    "    os.makedirs(os.path.join(dir_name, \"cosine_similarity\"), exist_ok=True)\n",
    "    def cosine_sim(a: np.ndarray, b: np.ndarray):\n",
    "        return round(dot(a, b)/(norm(a)*norm(b)), 6)\n",
    "    \n",
    "    if num_eval > len(baseline_obs):\n",
    "        num_eval = len(baseline_obs)\n",
    "    \n",
    "    for i in range(num_eval):\n",
    "        baseline_ep = baseline_obs[i]\n",
    "        baseline_x = list(range(len(baseline_ep)))\n",
    "        baseline_y = [cosine_sim(a, b) for (a,b) in zip(baseline_ep, baseline_ep)]\n",
    "        plt.plot(baseline_x, baseline_y, label = \"Baseline\")\n",
    "        \n",
    "        for j in range(len(target_model_names)):\n",
    "            target_ep = target_obs[j][i]\n",
    "            \n",
    "            if len(baseline_ep) < len(target_ep):\n",
    "                num_extra = len(target_ep) - len(baseline_ep)\n",
    "                baseline_ep = baseline_ep + [baseline_ep[-1]] * num_extra\n",
    "            \n",
    "            target_x = list(range(len(target_ep)))\n",
    "            target_y = [cosine_sim(a, b) for (a,b) in zip(target_ep, baseline_ep)]\n",
    "            plt.plot(target_x, target_y, label = target_model_names[j])\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Cosine Similarity')\n",
    "        \n",
    "        plt.title(f\"Policy Cosine Similarity\", \n",
    "                fontweight = \"bold\")\n",
    "        path = os.path.join(dir_name, \"cosine_similarity\", f\"episode_{i}\")\n",
    "        plt.savefig(path, bbox_inches=\"tight\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_obs = get_average_reward('baseline', num_episodes=1)\n",
    "# rand1_obs = get_average_reward('random', freq=1)\n",
    "# rand10_obs = get_average_reward('random', freq=10)\n",
    "adv1_obs = get_average_reward('adversarial', freq=1, num_episodes=1)\n",
    "adv10_obs = get_average_reward('adversarial', freq=10, num_episodes=1)\n",
    "pref50_obs = get_average_reward('preference', threshold=0.50, num_episodes=1)\n",
    "pref75_obs = get_average_reward('preference', threshold=0.75, num_episodes=1)\n",
    "pref90_obs = get_average_reward('preference', threshold=0.90, num_episodes=1)\n",
    "\n",
    "target_obs = [adv1_obs, adv10_obs, pref50_obs, pref75_obs, pref90_obs]\n",
    "names = [\"Adversarial_1\", \"Adversarial_10\", \n",
    "         \"Preference_0.5\", \"Preference_0.75\", \"Preference_0.9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "num_episodes = 10\n",
    "target_cluster = 23\n",
    "\n",
    "episode_rewards = []\n",
    "episode_attacks = []\n",
    "episode_steps = []\n",
    "episode_term_clusters = []\n",
    "episode_obs = []\n",
    "episode_perc_attack = []\n",
    "episode_action_deltas = []\n",
    "gif_lists = []\n",
    "\n",
    "dir_name = \"./outputs/attack/\"\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "print(\"Episode Number | Terminal Cluster | Total Reward | Number of Attacks\")\n",
    "for ep in range(num_episodes):\n",
    "    obs, _ = env.reset(seed=1234 + ep)\n",
    "    images = [Image.fromarray(env.render())]\n",
    "    ep_obs = [obs]\n",
    "    \n",
    "    done = False\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    n_attacks = 0\n",
    "    \n",
    "    reward = 0\n",
    "    while not done:\n",
    "        internal_data, _ = collector.collect_internal_data(obs)\n",
    "        preference, probs = split_data(internal_data)\n",
    "        \n",
    "        latent = internal_data.latent_actors\n",
    "        value = internal_data.critic_values\n",
    "        \n",
    "        data = np.concatenate([latent,\n",
    "                               np.expand_dims(value, axis=-1)\n",
    "                               ], axis=-1)\n",
    "        \n",
    "        if step == 0:\n",
    "            prediction = start_algo.predict(data.reshape(1, -1)) + 20\n",
    "        else:\n",
    "            prediction = mid_algo.predict(data.reshape(1, -1))\n",
    "        \n",
    "        if prediction == 7:\n",
    "            action = 2\n",
    "            n_attacks += 1\n",
    "            gt_action = np.argmax(probs).item()\n",
    "            action_delta = probs[gt_action] - probs[action]\n",
    "            episode_action_deltas.append(action_delta)\n",
    "        elif prediction in [16, 0, 8]:\n",
    "            action = 3\n",
    "            n_attacks += 1\n",
    "            gt_action = np.argmax(probs).item()\n",
    "            action_delta = probs[gt_action] - probs[action]\n",
    "            episode_action_deltas.append(action_delta)\n",
    "        elif prediction in [11,12]:\n",
    "            action = 1\n",
    "            n_attacks += 1\n",
    "            gt_action = np.argmax(probs).item()\n",
    "            action_delta = probs[gt_action] - probs[action]\n",
    "            episode_action_deltas.append(action_delta)\n",
    "        else:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        images.append(Image.fromarray(env.render()))\n",
    "        ep_obs.append(obs)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        step += 1\n",
    "\n",
    "    gif_lists.append(images)\n",
    "    \n",
    "    data = np.concatenate([latent,\n",
    "                           np.expand_dims(value, axis=-1),\n",
    "                           np.expand_dims(reward, axis=-1),\n",
    "                           ], axis=-1)\n",
    "    \n",
    "    prediction = term_algo.predict(data.reshape(1, -1)).item() + 22\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_attacks.append(n_attacks)\n",
    "    episode_steps.append(step)\n",
    "    episode_term_clusters.append(prediction)\n",
    "    episode_obs.append(ep_obs)\n",
    "    episode_perc_attack.append(n_attacks / step)\n",
    "\n",
    "# save_gifs(os.path.join(dir_name, 'gifs', \"arlin\"), gif_lists, episode_rewards)\n",
    "avg_reward, avg_attacks, avg_perc_attack = get_averages(num_episodes,\n",
    "                                                        episode_rewards,\n",
    "                                                        episode_attacks,\n",
    "                                                        episode_perc_attack)\n",
    "action_delta_histogram(episode_action_deltas, \"Action Deltas for ARLIN\", os.path.join(dir_name, \"metrics/arlin\"))\n",
    "\n",
    "target_perc = (sum([i == 23 for i in episode_term_clusters]) / num_episodes) * 100\n",
    "\n",
    "print(f\"Avg Reward: {avg_reward} | Avg Num Attacks: {avg_attacks} | Avg Percent Attacks: {avg_perc_attack} | Target Reached %: {target_perc}\")\n",
    "\n",
    "plot_cosine_sim(baseline_obs, target_obs + [episode_obs], names + [\"ARLIN\"], './outputs/attack/metrics')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
