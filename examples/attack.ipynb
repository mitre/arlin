{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Attack\n",
    "\n",
    "This notebook looks into how ARLIN can be used to create more effective adversarial attacks. The notebook will show the average reward gained and total number of attacks in various attack scenarios against the same trained RL model:\n",
    "\n",
    "- Random action every step\n",
    "- Worst-case action every step\n",
    "- Worst-case action every 10 steps\n",
    "- Least-preferred action based on threshold (https://arxiv.org/pdf/1703.06748.pdf)\n",
    "- ARLIN-informed actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import arlin.dataset.loaders as loaders\n",
    "from arlin.dataset import XRLDataset\n",
    "from arlin.dataset.collectors import SB3PPODataCollector, SB3PPODatapoint\n",
    "\n",
    "from arlin.generation import generate_clusters, generate_embeddings\n",
    "import arlin.analysis.visualization as viz\n",
    "from arlin.analysis import ClusterAnalyzer, LatentAnalyzer\n",
    "from arlin.samdp import SAMDP\n",
    "import arlin.utils.saving_loading as sl_utils\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode='rgb_array')\n",
    "\n",
    "# Load the SB3 model from Huggingface\n",
    "model = loaders.load_hf_sb_model(repo_id=\"sb3/ppo-LunarLander-v2\",\n",
    "                                 filename=\"ppo-LunarLander-v2.zip\",\n",
    "                                 algo_str=\"ppo\")\n",
    "\n",
    "adv_model = loaders.load_sb_model('./models/adv_ppo_lunar.zip', 'ppo')\n",
    "\n",
    "# Create the datapoint collector for SB3 PPO Datapoints with the model's policy\n",
    "collector = SB3PPODataCollector(datapoint_cls=SB3PPODatapoint,\n",
    "                                policy=model.policy)\n",
    "\n",
    "# Instantiate the XRL Dataset\n",
    "dataset = XRLDataset(env, collector=collector)\n",
    "\n",
    "# dataset.fill(num_datapoints=50000)\n",
    "# dataset.save(file_path='./data/LunarLander-50000.npz')\n",
    "\n",
    "# Load the dataset, embeddings, and clusters\n",
    "dataset.load('./data/LunarLander-50000.npz')\n",
    "embeddings = sl_utils.load_data(file_path='./data/LunarLander-50000-Embeddings.npy')\n",
    "# clusters = sl_utils.load_data(file_path='./data/LunarLander-50000-Clusters.npy')\n",
    "\n",
    "clusters, start_algo, term_algo, mid_algo = generate_clusters(dataset=dataset,\n",
    "                                     embeddings=embeddings,\n",
    "                                     num_clusters=14)\n",
    "        \n",
    "sl_utils.save_data(data=[start_algo, term_algo, mid_algo], file_path='./models/cluster_algos.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARLIN Usage\n",
    "\n",
    "Let's use the ARLIN toolkit to identify when we should be performing our adversarial\n",
    "attack, and which actions we should target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_latent_analytics(embeddings: np.ndarray, \n",
    "                           clusters: np.ndarray, \n",
    "                           dataset: XRLDataset):\n",
    "    \"\"\"Graph visualizations of different latent space analytics over embeddings.\"\"\"\n",
    "    \n",
    "    # Create a grapher to generate data used for analysis.\n",
    "    grapher = LatentAnalyzer(embeddings, dataset)\n",
    "    \n",
    "    # Clusters\n",
    "    cluster_data = grapher.clusters_graph_data(clusters)\n",
    "    # Episode progression\n",
    "    ep_prog_data = grapher.episode_prog_graph_data()\n",
    "    # Greedy action confidence\n",
    "    conf_data = grapher.confidence_data()\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", \"latent_analytics\")\n",
    "    \n",
    "    # Graph multiple analytics as subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path,\n",
    "                                           figure_title='Latent Analytics', \n",
    "                                           graph_datas=[conf_data, \n",
    "                                                        cluster_data, \n",
    "                                                        ep_prog_data])\n",
    "\n",
    "def graph_cluster_analytics(dataset, clusters):\n",
    "    \"\"\"Graph analytics for each cluster\"\"\"\n",
    "    \n",
    "    # Create grapher to graph cluster analytics\n",
    "    grapher = ClusterAnalyzer(dataset, clusters)\n",
    "    \n",
    "    grapher.cluster_state_analysis(18,\n",
    "                                   env, \n",
    "                                   os.path.join(\".\", \"outputs\", \"attack\", \"cluster_state_analysis\"))\n",
    "    \n",
    "    grapher.cluster_state_analysis(9,\n",
    "                                   env, \n",
    "                                   os.path.join(\".\", \"outputs\", \"attack\", \"cluster_state_analysis\"))\n",
    "    \n",
    "    grapher.cluster_state_analysis(8,\n",
    "                                   env, \n",
    "                                   os.path.join(\".\", \"outputs\", \"attack\", \"cluster_state_analysis\"))\n",
    "    \n",
    "    # Mean confidence per cluster\n",
    "    cluster_conf = grapher.cluster_confidence()\n",
    "    # Mean total reward per cluster\n",
    "    cluster_rewards = grapher.cluster_rewards()\n",
    "    # Mean value per cluster\n",
    "    cluster_values = grapher.cluster_values()\n",
    "    \n",
    "    # Graph individual graphs per data\n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", 'cluster_analytics')\n",
    "    \n",
    "    # Graph multiple subplots in one plot\n",
    "    combined_path = os.path.join(base_path, 'combined_analytics.png')\n",
    "    viz.graph_multiple_data(file_path=combined_path, \n",
    "                                           figure_title='Cluster Analytics', \n",
    "                                           graph_datas=[cluster_conf,\n",
    "                                                        cluster_values,\n",
    "                                                        cluster_rewards])\n",
    "\n",
    "def samdp(clusters: np.ndarray,\n",
    "          dataset: XRLDataset):\n",
    "    \"\"\"Generate a semi-aggregated Markov decision process.\"\"\"\n",
    "    \n",
    "    # Create the SAMDP\n",
    "    samdp = SAMDP(clusters, dataset)\n",
    "    \n",
    "    base_path = os.path.join(\".\", \"outputs\", \"attack\", 'samdp')\n",
    "    \n",
    "    # Simplified graph with all possible connections (regardless of action taken)\n",
    "    simplified_graph = samdp.save_simplified_graph(f'{base_path}/samdp_simplified.png')\n",
    "    \n",
    "    path_path = os.path.join(base_path, f\"samdp_path_15_18\")\n",
    "    \n",
    "    # Path from cluster 15 to cluster 18\n",
    "    # Only the most likely path is shown\n",
    "    samdp.save_paths(15, \n",
    "                     18,  \n",
    "                     f'{os.path.join(base_path, f\"samdp_path_15_18\")}_bp.png', \n",
    "                     best_path_only=True)\n",
    "    \n",
    "    # Path from cluster 9 to cluster 18\n",
    "    # Only the most likely path is shown\n",
    "    samdp.save_paths(9, \n",
    "                     18,  \n",
    "                     f'{os.path.join(base_path, f\"samdp_path_9_18\")}_bp.png', \n",
    "                     best_path_only=True)\n",
    "    \n",
    "    # Show all paths that lead to cluster 18\n",
    "    # Action into cluster 18 shown, rest is simplified\n",
    "    samdp.save_all_paths_to(18, \n",
    "                            os.path.join(base_path, f\"samdp_paths_to_18.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_latent_analytics(embeddings, clusters, dataset)\n",
    "graph_cluster_analytics(dataset, clusters)\n",
    "samdp(clusters, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_attack(model_type: str, \n",
    "                  timestep: int, \n",
    "                  freq: int = 0,\n",
    "                  preference: float = 0, \n",
    "                  threshold: float = 1.0) -> bool:\n",
    "    \"\"\"Check whether or not we should attack at the given timestep.\n",
    "\n",
    "    Args:\n",
    "        model_type (str): Type of model we want to run.\n",
    "        timestep (int): Current timestep\n",
    "        freq (int, optional): Frequency of attack. Defaults to 0.\n",
    "        preference (float, optional): Delta between most and least preferred action.\n",
    "            Defaults to 0.\n",
    "        threshold (float, optional): Threshold for preference attack. Defaults to 1.0.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If invalid model type is given.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether or not to attack\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_type == 'baseline':\n",
    "        return False\n",
    "    elif model_type == 'random' or model_type == 'adversarial':\n",
    "        if timestep % freq == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif model_type == 'preference':\n",
    "        if preference > threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model_type {model_type} given.\")\n",
    "\n",
    "def get_action(obs: np.ndarray,\n",
    "               model_type: str, \n",
    "               timestep: int, \n",
    "               freq: int = 0,\n",
    "               preference: float = 0, \n",
    "               threshold: float = 1.0) -> int:\n",
    "    \"\"\"Get the action to take at the given timestep.\n",
    "\n",
    "    Args:\n",
    "        obs (np.ndarray): Current observation from the agent.\n",
    "        model_type (str): Type of model we want to run.\n",
    "        timestep (int): Current timestep\n",
    "        freq (int, optional): Frequency of attack. Defaults to 0.\n",
    "        preference (float, optional): Delta between most and least preferred action.\n",
    "            Defaults to 0.\n",
    "        threshold (float, optional): Threshold for preference attack. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        int: Action value to take.\n",
    "    \"\"\"\n",
    "    adv = False\n",
    "    if should_attack(model_type, timestep, freq, preference, threshold):\n",
    "        adv = True\n",
    "        if model_type == 'random':\n",
    "            rng = np.random.default_rng(12345)\n",
    "            action = rng.integers(low=0, high=env.action_space.n, size=1).item()\n",
    "        else:\n",
    "            action, _ = adv_model.predict(obs, deterministic=True)\n",
    "    else:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "    return action, adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_reward(model_type: str ='baseline', \n",
    "                       freq: int = 0,\n",
    "                       threshold: int = 1) -> float:\n",
    "    \"\"\"Average reward over 10 episodes while the model is being attacked.\n",
    "    \n",
    "    Attacks happen at the given freq and come from the given model type. \n",
    "        - Baseline does not include any adversarial attacks.\n",
    "        - Random chooses the action randomly.\n",
    "        - Adversarial chooses the worst possible action at that point in time.\n",
    "        - Preference chooses the least preferred action when the pref is above a threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_attacks = []\n",
    "    \n",
    "    for ep in range(10):\n",
    "        obs, _ = env.reset(seed=1234 + ep)\n",
    "        done = False\n",
    "        step = 0\n",
    "        ep_rew = 0\n",
    "        adv_attacks = 0\n",
    "        \n",
    "        while not done:\n",
    "            internal_data, _ = collector.collect_internal_data(obs)\n",
    "            probs = internal_data.dist_probs\n",
    "            preference = probs.max() - probs.min()\n",
    "            \n",
    "            action, adv = get_action(obs, model_type, step, freq, preference, threshold)\n",
    "            if adv:\n",
    "                adv_attacks += 1\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            ep_rew += reward\n",
    "            done = terminated or truncated\n",
    "            step += 1\n",
    "            \n",
    "        episode_rewards.append(ep_rew)\n",
    "        episode_attacks.append(adv_attacks)\n",
    "    \n",
    "    return sum(episode_rewards) / 10, sum(episode_attacks) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline, base_n_attack = get_average_reward('baseline')\n",
    "rand_every_1, rand1_n_attack = get_average_reward('random', freq=1)\n",
    "rand_every_10, rand10_n_attack = get_average_reward('random', freq=10)\n",
    "worst_every_1, worst1_n_attack = get_average_reward('adversarial', freq=1)\n",
    "worst_every_10, worst10_n_attack = get_average_reward('adversarial', freq=10)\n",
    "preference_50, pref50_n_attack = get_average_reward('preference', threshold=0.50)\n",
    "preference_75, pref75_n_attack = get_average_reward('preference', threshold=0.75)\n",
    "preference_90, pref90_n_attack = get_average_reward('preference', threshold=0.9)\n",
    "\n",
    "print(f\"Baseline Avg Reward: {baseline} with {base_n_attack} attacks\")\n",
    "print(f\"Random Action Every 1 Avg Reward: {rand_every_1} with {rand1_n_attack} attacks\")\n",
    "print(f\"Random Action Every 10 Avg Reward: {rand_every_10} with {rand10_n_attack} attacks\")\n",
    "print(f\"Worst Action Every 1 Avg Reward: {worst_every_1} with {worst1_n_attack} attacks\")\n",
    "print(f\"Worst Action Every 10 Avg Reward: {worst_every_10} with {worst10_n_attack} attacks\")\n",
    "print(f\"Preference at .50 Avg Reward: {preference_50} with {pref50_n_attack} attacks\")\n",
    "print(f\"Preference at .75 Avg Reward: {preference_75} with {pref75_n_attack} attacks\")\n",
    "print(f\"Preference at .90 Avg Reward: {preference_90} with {pref90_n_attack} attacks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
